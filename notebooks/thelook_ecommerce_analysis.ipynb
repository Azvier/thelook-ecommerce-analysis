{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfde04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f61687",
   "metadata": {},
   "source": [
    "# Database Connection\n",
    "\n",
    "Connect to the PostgreSQL database with the provided credentials. SQLAlchemy will be used for database operations in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ae5e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLAlchemy engine created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Connect to PostgreSQL using SQLAlchemy\n",
    "try:\n",
    "    engine = create_engine(\n",
    "        'postgresql://azalea:azalea@localhost:5433/thelook_db'\n",
    "    )\n",
    "    # Test the connection\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text('SELECT 1'))\n",
    "    print(\"SQLAlchemy engine created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to database: {e}\")\n",
    "    print(\"\\nPlease ensure that PostgreSQL database is running\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2248b7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['users',\n",
       " 'events',\n",
       " 'orders',\n",
       " 'distribution_centers',\n",
       " 'products',\n",
       " 'inventory_items',\n",
       " 'order_items']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe97d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Read each table into a pandas DataFrame, assigning to individual variables\n",
    "users = pd.read_sql_table('users', engine)\n",
    "events = pd.read_sql_table('events', engine)\n",
    "orders = pd.read_sql_table('orders', engine)\n",
    "distribution_centers = pd.read_sql_table('distribution_centers', engine)\n",
    "products = pd.read_sql_table('products', engine)\n",
    "inventory_items = pd.read_sql_table('inventory_items', engine)\n",
    "order_items = pd.read_sql_table('order_items', engine)\n",
    "\n",
    "print('All tables read successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b9c9455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLAlchemy engine disposed.\n"
     ]
    }
   ],
   "source": [
    "# Close all database connections\n",
    "try:\n",
    "    if 'engine' in locals():\n",
    "        engine.dispose()\n",
    "        print(\"SQLAlchemy engine disposed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error disposing SQLAlchemy engine: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a20e54",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a76d0",
   "metadata": {},
   "source": [
    "## Data Overview and Quality Assessment\n",
    "\n",
    "Understanding the structure and quality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7d2a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shapes:\n",
      "Users: (100000, 16)\n",
      "Events: (2428216, 13)\n",
      "Orders: (125278, 9)\n",
      "Order Items: (181578, 11)\n",
      "Products: (29120, 9)\n",
      "Inventory Items: (490176, 12)\n",
      "Distribution Centers: (10, 5)\n",
      "\n",
      "Memory Usage (MB):\n",
      "users: 67.77 MBusers: 67.77 MB\n",
      "events: 1362.93 MB\n",
      "orders: 19.50 MB\n",
      "order_items: 23.77 MB\n",
      "products: 10.62 MB\n",
      "inventory_items: 189.79 MB\n",
      "distribution_centers: 0.00 MB\n",
      "\n",
      "events: 1362.93 MB\n",
      "orders: 19.50 MB\n",
      "order_items: 23.77 MB\n",
      "products: 10.62 MB\n",
      "inventory_items: 189.79 MB\n",
      "distribution_centers: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Basic information about each dataset\n",
    "print(\"Dataset Shapes:\")\n",
    "print(f\"Users: {users.shape}\")\n",
    "print(f\"Events: {events.shape}\")\n",
    "print(f\"Orders: {orders.shape}\")\n",
    "print(f\"Order Items: {order_items.shape}\")\n",
    "print(f\"Products: {products.shape}\")\n",
    "print(f\"Inventory Items: {inventory_items.shape}\")\n",
    "print(f\"Distribution Centers: {distribution_centers.shape}\")\n",
    "\n",
    "# Memory usage\n",
    "print(\"\\nMemory Usage (MB):\")\n",
    "for name, df in [('users', users), ('events', events), ('orders', orders), \n",
    "                 ('order_items', order_items), ('products', products), \n",
    "                 ('inventory_items', inventory_items), ('distribution_centers', distribution_centers)]:\n",
    "    print(f\"{name}: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d37a6",
   "metadata": {},
   "source": [
    "The datasets are large and detailed, especially `events`, which uses the most memory. Most tables have hundreds of thousands of rows, except for `distribution_centers`, which is small. Efficient filtering is important for analysis due to the size of the largest tables. Overall, the data is comprehensive and ready for business analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd48311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== USERS DATA QUALITY ===\n",
      "Shape: (100000, 16)\n",
      "Duplicates: 0\n",
      "\n",
      "No missing values found!\n",
      "\n",
      "Data Types:\n",
      "object                 11\n",
      "int64                   2\n",
      "float64                 2\n",
      "datetime64[ns, UTC]     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ORDERS DATA QUALITY ===\n",
      "Shape: (125278, 9)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "              Missing Count  Missing %\n",
      "returned_at          112833  90.066093\n",
      "delivered_at          81219  64.831016\n",
      "shipped_at            43559  34.769872\n",
      "\n",
      "Data Types:\n",
      "datetime64[ns, UTC]    4\n",
      "int64                  3\n",
      "object                 2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ORDER_ITEMS DATA QUALITY ===\n",
      "Shape: (181578, 11)\n",
      "Duplicates: 0\n",
      "\n",
      "No missing values found!\n",
      "\n",
      "Data Types:\n",
      "object                 11\n",
      "int64                   2\n",
      "float64                 2\n",
      "datetime64[ns, UTC]     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ORDERS DATA QUALITY ===\n",
      "Shape: (125278, 9)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "              Missing Count  Missing %\n",
      "returned_at          112833  90.066093\n",
      "delivered_at          81219  64.831016\n",
      "shipped_at            43559  34.769872\n",
      "\n",
      "Data Types:\n",
      "datetime64[ns, UTC]    4\n",
      "int64                  3\n",
      "object                 2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ORDER_ITEMS DATA QUALITY ===\n",
      "Shape: (181578, 11)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "              Missing Count  Missing %\n",
      "returned_at          163615  90.107282\n",
      "delivered_at         117660  64.798599\n",
      "shipped_at            63029  34.711804\n",
      "\n",
      "Data Types:\n",
      "int64                  5\n",
      "datetime64[ns, UTC]    4\n",
      "object                 1\n",
      "float64                1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== PRODUCTS DATA QUALITY ===\n",
      "Shape: (29120, 9)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "       Missing Count  Missing %\n",
      "brand             24   0.082418\n",
      "name               2   0.006868\n",
      "\n",
      "Data Types:\n",
      "object     5\n",
      "int64      2\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== INVENTORY_ITEMS DATA QUALITY ===\n",
      "Shape: (490176, 12)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "              Missing Count  Missing %\n",
      "returned_at          163615  90.107282\n",
      "delivered_at         117660  64.798599\n",
      "shipped_at            63029  34.711804\n",
      "\n",
      "Data Types:\n",
      "int64                  5\n",
      "datetime64[ns, UTC]    4\n",
      "object                 1\n",
      "float64                1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== PRODUCTS DATA QUALITY ===\n",
      "Shape: (29120, 9)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "       Missing Count  Missing %\n",
      "brand             24   0.082418\n",
      "name               2   0.006868\n",
      "\n",
      "Data Types:\n",
      "object     5\n",
      "int64      2\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== INVENTORY_ITEMS DATA QUALITY ===\n",
      "Shape: (490176, 12)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "         Missing Count  Missing %\n",
      "sold_at         308598  62.956571\n",
      "\n",
      "Data Types:\n",
      "object                 5\n",
      "int64                  3\n",
      "datetime64[ns, UTC]    2\n",
      "float64                2\n",
      "Name: count, dtype: int64\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "         Missing Count  Missing %\n",
      "sold_at         308598  62.956571\n",
      "\n",
      "Data Types:\n",
      "object                 5\n",
      "int64                  3\n",
      "datetime64[ns, UTC]    2\n",
      "float64                2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data quality assessment\n",
    "import numpy as np\n",
    "\n",
    "def data_quality_summary(df, name):\n",
    "    print(f\"\\n=== {name.upper()} DATA QUALITY ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    })\n",
    "    missing_info = missing_info[missing_info['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    if not missing_info.empty:\n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(missing_info)\n",
    "    else:\n",
    "        print(\"\\nNo missing values found!\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    return missing_info\n",
    "\n",
    "# Check each dataset\n",
    "for name, df in [('users', users), ('orders', orders), ('order_items', order_items), \n",
    "                 ('products', products), ('inventory_items', inventory_items)]:\n",
    "    data_quality_summary(df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a66311",
   "metadata": {},
   "source": [
    "The data quality assessment shows:\n",
    "\n",
    "- **Missing Values**: Minimal missing data across most tables. Geographic fields (latitude, longitude) and fulfillment dates (shipped_at, delivered_at) show expected gaps. Core business fields (IDs, prices, essential dates) are complete.\n",
    "\n",
    "- **Data Types**: Appropriate data types throughout - numeric fields for calculations, text fields for categories, and timestamp fields for time series analysis.\n",
    "\n",
    "- **Duplicates**: No duplicate records found, indicating proper data deduplication.\n",
    "\n",
    "- **Overall**: The data quality is excellent for business analysis with minimal cleaning required. The completeness of critical fields enables comprehensive examination of customer behavior, product performance, and operational metrics.\n",
    "\n",
    "Given the missing data patterns, we need to investigate whether these are truly missing values or represent natural business states (e.g., orders not yet shipped, products not yet sold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e2498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MISSING DATA MANAGEMENT STRATEGY ===\n",
      "\n",
      "1. ORDERS TABLE - Analyzing fulfillment workflow missing data\n",
      "\n",
      "Missing fulfillment dates by order status:\n",
      "            Missing_Shipped  Missing_Delivered  Missing_Returned  Total_Orders\n",
      "status                                                                        \n",
      "Cancelled             18714              18714             18714         18714\n",
      "Complete                  0                  0             31614         31614\n",
      "Processing            24845              24845             24845         24845\n",
      "Returned                  0                  0                 0         12445\n",
      "Shipped                   0              37660             37660         37660\n",
      "\n",
      "Missing fulfillment dates by order status (with percentages):\n",
      "            Total_Orders  Missing_Shipped  Missing_Shipped_Pct  \\\n",
      "status                                                           \n",
      "Cancelled          18714            18714                100.0   \n",
      "Complete           31614                0                  0.0   \n",
      "Processing         24845            24845                100.0   \n",
      "Returned           12445                0                  0.0   \n",
      "Shipped            37660                0                  0.0   \n",
      "\n",
      "            Missing_Delivered  Missing_Delivered_Pct  Missing_Returned  \\\n",
      "status                                                                   \n",
      "Cancelled               18714                  100.0             18714   \n",
      "Complete                    0                    0.0             31614   \n",
      "Processing              24845                  100.0             24845   \n",
      "Returned                    0                    0.0                 0   \n",
      "Shipped                 37660                  100.0             37660   \n",
      "\n",
      "            Missing_Returned_Pct  \n",
      "status                            \n",
      "Cancelled                  100.0  \n",
      "Complete                   100.0  \n",
      "Processing                 100.0  \n",
      "Returned                     0.0  \n",
      "Shipped                    100.0  \n"
     ]
    }
   ],
   "source": [
    "# Missing Data Management and Analysis\n",
    "print(\"=== MISSING DATA MANAGEMENT STRATEGY ===\")\n",
    "print(\"\\n1. ORDERS TABLE - Analyzing fulfillment workflow missing data\")\n",
    "\n",
    "# Check if missing dates are due to order status rather than true missing data\n",
    "order_status_analysis = orders.groupby('status').agg({\n",
    "    'shipped_at': lambda x: x.isnull().sum(),\n",
    "    'delivered_at': lambda x: x.isnull().sum(), \n",
    "    'returned_at': lambda x: x.isnull().sum()\n",
    "}).astype(int)\n",
    "\n",
    "order_status_analysis.columns = ['Missing_Shipped', 'Missing_Delivered', 'Missing_Returned']\n",
    "order_status_analysis['Total_Orders'] = orders.groupby('status').size()\n",
    "\n",
    "print(\"\\nMissing fulfillment dates by order status:\")\n",
    "print(order_status_analysis)\n",
    "\n",
    "# Calculate percentages\n",
    "for col in ['Missing_Shipped', 'Missing_Delivered', 'Missing_Returned']:\n",
    "    order_status_analysis[f'{col}_Pct'] = (order_status_analysis[col] / order_status_analysis['Total_Orders'] * 100).round(1)\n",
    "\n",
    "print(\"\\nMissing fulfillment dates by order status (with percentages):\")\n",
    "print(order_status_analysis[['Total_Orders', 'Missing_Shipped', 'Missing_Shipped_Pct', \n",
    "                           'Missing_Delivered', 'Missing_Delivered_Pct',\n",
    "                           'Missing_Returned', 'Missing_Returned_Pct']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0096eff",
   "metadata": {},
   "source": [
    "Analysis reveals that missing fulfillment dates represent the natural order processing workflow rather than data quality issues. Each status shows expected patterns:\n",
    "\n",
    "- **Cancelled orders**: 100% missing all dates (never entered fulfillment)\n",
    "- **Processing orders**: 100% missing all dates (awaiting shipment)\n",
    "- **Shipped orders**: Have shipping dates but 100% missing delivered/returned dates\n",
    "- **Complete orders**: No missing shipping/delivery dates, but 100% missing return dates\n",
    "- **Returned orders**: Complete data across all date fields\n",
    "\n",
    "These patterns confirm the database accurately tracks order lifecycle stages, with \"missing\" values actually serving as meaningful status indicators rather than data deficiencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e889b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. ORDER_ITEMS TABLE - Analyzing item-level fulfillment data\n",
      "\n",
      "Missing fulfillment dates in order_items by status:\n",
      "            Missing_Shipped  Missing_Delivered  Missing_Returned  Total_Items\n",
      "status                                                                       \n",
      "Cancelled             27190              27190             27190        27190\n",
      "Complete                  0                  0             45955        45955\n",
      "Processing            35839              35839             35839        35839\n",
      "Returned                  0                  0                 0        17963\n",
      "Shipped                   0              54631             54631        54631\n",
      "\n",
      "\n",
      "3. INVENTORY_ITEMS TABLE - Analyzing sold_at missing data\n",
      "\n",
      "Inventory items sold status:\n",
      "   Total_Inventory_Items  Items_with_sold_at  Items_NOT_sold  \\\n",
      "0                 490176              181578          308598   \n",
      "\n",
      "   Percentage_Unsold  \n",
      "0               63.0  \n",
      "\n",
      "Items with sold_at: 181,578\n",
      "Items NOT sold: 308,598\n",
      "Sum of both: 490,176\n",
      "Total inventory items (verification): 490,176\n",
      "\n",
      "Verification: Sum equals total inventory items: True\n"
     ]
    }
   ],
   "source": [
    "print(\"2. ORDER_ITEMS TABLE - Analyzing item-level fulfillment data\")\n",
    "\n",
    "# Analyze missing data in order_items by status\n",
    "order_items_analysis = order_items.groupby('status').agg({\n",
    "    'shipped_at': lambda x: x.isnull().sum(),\n",
    "    'delivered_at': lambda x: x.isnull().sum(),\n",
    "    'returned_at': lambda x: x.isnull().sum()\n",
    "}).astype(int)\n",
    "\n",
    "order_items_analysis.columns = ['Missing_Shipped', 'Missing_Delivered', 'Missing_Returned']\n",
    "order_items_analysis['Total_Items'] = order_items.groupby('status').size()\n",
    "\n",
    "print(\"\\nMissing fulfillment dates in order_items by status:\")\n",
    "print(order_items_analysis)\n",
    "\n",
    "# Check if order_items missing data aligns with parent orders\n",
    "print(\"\\n\\n3. INVENTORY_ITEMS TABLE - Analyzing sold_at missing data\")\n",
    "\n",
    "# For inventory_items, missing sold_at likely means items haven't been sold yet\n",
    "inventory_sold_analysis = pd.DataFrame({\n",
    "    'Total_Inventory_Items': [len(inventory_items)],\n",
    "    'Items_with_sold_at': [inventory_items['sold_at'].notna().sum()],\n",
    "    'Items_NOT_sold': [inventory_items['sold_at'].isnull().sum()],\n",
    "    'Percentage_Unsold': [(inventory_items['sold_at'].isnull().sum() / len(inventory_items) * 100).round(1)]\n",
    "})\n",
    "\n",
    "print(\"\\nInventory items sold status:\")\n",
    "print(inventory_sold_analysis)\n",
    "\n",
    "# Calculate and print the sum of Items_with_sold_at and Items_NOT_sold\n",
    "items_sold = inventory_sold_analysis['Items_with_sold_at'].iloc[0]\n",
    "items_not_sold = inventory_sold_analysis['Items_NOT_sold'].iloc[0]\n",
    "total_sum = items_sold + items_not_sold\n",
    "\n",
    "print(f\"\\nItems with sold_at: {items_sold:,}\")\n",
    "print(f\"Items NOT sold: {items_not_sold:,}\")\n",
    "print(f\"Sum of both: {total_sum:,}\")\n",
    "print(f\"Total inventory items (verification): {len(inventory_items):,}\")\n",
    "print(f\"\\nVerification: Sum equals total inventory items: {total_sum == len(inventory_items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0480df3",
   "metadata": {},
   "source": [
    "**Order Items Processing Flow**\n",
    "- **Cancelled orders**: 100% missing all fulfillment dates (27,190 items)\n",
    "- **Processing orders**: 100% missing all dates as they haven't been shipped (35,839 items)\n",
    "- **Shipped orders**: Have shipping dates but await delivery (54,631 items)\n",
    "- **Complete orders**: Full shipping and delivery information (45,955 items) \n",
    "- **Returned items**: Complete history across all date fields (17,963 items)\n",
    "\n",
    "**Inventory Management Status**\n",
    "- **37% of inventory sold**: 181,578 items have sold_at dates\n",
    "- **63% of inventory unsold**: 308,598 items remain available in stock\n",
    "- **Total inventory**: 490,176 items\n",
    "\n",
    "These patterns indicate that missing date values represent meaningful business states in the order processing and inventory management workflows rather than data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0255199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. PRODUCTS TABLE - Analyzing missing product data\n",
      "\n",
      "Missing data in products table:\n",
      "       Missing_Count  Missing_Percentage\n",
      "brand             24                0.08\n",
      "name               2                0.01\n",
      "\n",
      "Analyzing patterns in missing product data:\n",
      "\n",
      "brand missing by category (top 5):\n",
      "category\n",
      "Intimates            4\n",
      "Tops & Tees          4\n",
      "Outerwear & Coats    3\n",
      "Swim                 2\n",
      "Accessories          2\n",
      "Name: brand, dtype: int64\n",
      "\n",
      "brand missing by department:\n",
      "department\n",
      "Men      12\n",
      "Women    12\n",
      "Name: brand, dtype: int64\n",
      "\n",
      "name missing by category (top 5):\n",
      "category\n",
      "Intimates            1\n",
      "Outerwear & Coats    1\n",
      "Name: name, dtype: int64\n",
      "\n",
      "name missing by department:\n",
      "department\n",
      "Men      1\n",
      "Women    1\n",
      "Name: name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"4. PRODUCTS TABLE - Analyzing missing product data\")\n",
    "\n",
    "# Analyze missing data patterns in products\n",
    "products_missing = products.isnull().sum()\n",
    "products_missing_pct = (products_missing / len(products) * 100).round(2)\n",
    "\n",
    "products_missing_df = pd.DataFrame({\n",
    "    'Missing_Count': products_missing,\n",
    "    'Missing_Percentage': products_missing_pct\n",
    "})\n",
    "products_missing_df = products_missing_df[products_missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"\\nMissing data in products table:\")\n",
    "print(products_missing_df)\n",
    "\n",
    "# Check if missing data correlates with certain categories or departments\n",
    "if not products_missing_df.empty:\n",
    "    print(\"\\nAnalyzing patterns in missing product data:\")\n",
    "    \n",
    "    # Check missing data by category\n",
    "    for col in products_missing_df.index:\n",
    "        if col in products.columns:\n",
    "            missing_by_category = products.groupby('category')[col].apply(lambda x: x.isnull().sum())\n",
    "            missing_by_category = missing_by_category[missing_by_category > 0].sort_values(ascending=False)\n",
    "            \n",
    "            if not missing_by_category.empty:\n",
    "                print(f\"\\n{col} missing by category (top 5):\")\n",
    "                print(missing_by_category.head())\n",
    "            \n",
    "            # Check missing data by department\n",
    "            missing_by_dept = products.groupby('department')[col].apply(lambda x: x.isnull().sum())\n",
    "            missing_by_dept = missing_by_dept[missing_by_dept > 0].sort_values(ascending=False)\n",
    "            \n",
    "            if not missing_by_dept.empty:\n",
    "                print(f\"\\n{col} missing by department:\")\n",
    "                print(missing_by_dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8484b",
   "metadata": {},
   "source": [
    "The products table shows excellent data quality with very limited missing information. The few missing values are evenly distributed across departments, suggesting random omissions rather than systematic data issues. table shows excellent data quality with very limited missing information. The few missing values are evenly distributed across departments, suggesting random omissions rather than systematic data issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0056a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATE RANGES ===\n",
      "\n",
      "USERS:\n",
      "  created_at: 02 Jan 2019 to 24 May 2025\n",
      "    Range: 2334 days\n",
      "\n",
      "EVENTS:\n",
      "  created_at: 02 Jan 2019 to 29 May 2025\n",
      "    Range: 2339 days\n",
      "\n",
      "ORDERS:\n",
      "  created_at: 06 Jan 2019 to 25 May 2025\n",
      "    Range: 2331 days\n",
      "  returned_at: 25 Jan 2019 to 04 Jun 2025\n",
      "    Range: 2321 days\n",
      "  shipped_at: 06 Jan 2019 to 28 May 2025\n",
      "    Range: 2334 days\n",
      "  delivered_at: 13 Jan 2019 to 02 Jun 2025\n",
      "    Range: 2331 days\n",
      "\n",
      "ORDER_ITEMS:\n",
      "  created_at: 06 Jan 2019 to 29 May 2025\n",
      "    Range: 2335 days\n",
      "  shipped_at: 06 Jan 2019 to 28 May 2025\n",
      "    Range: 2334 days\n",
      "  delivered_at: 13 Jan 2019 to 02 Jun 2025\n",
      "    Range: 2331 days\n",
      "  returned_at: 25 Jan 2019 to 04 Jun 2025\n",
      "    Range: 2321 days\n",
      "\n",
      "INVENTORY_ITEMS:\n",
      "  created_at: 21 Nov 2018 to 29 May 2025\n",
      "    Range: 2380 days\n",
      "  sold_at: 06 Jan 2019 to 29 May 2025\n",
      "    Range: 2335 days\n",
      "\n",
      "=== OVERALL DATE RANGE ===\n",
      "\n",
      "Oldest date: 21 Nov 2018 (from inventory_items.created_at)\n",
      "Newest date: 04 Jun 2025 (from orders.returned_at)\n",
      "Total time span: 2386 days\n",
      "  delivered_at: 13 Jan 2019 to 02 Jun 2025\n",
      "    Range: 2331 days\n",
      "  returned_at: 25 Jan 2019 to 04 Jun 2025\n",
      "    Range: 2321 days\n",
      "\n",
      "INVENTORY_ITEMS:\n",
      "  created_at: 21 Nov 2018 to 29 May 2025\n",
      "    Range: 2380 days\n",
      "  sold_at: 06 Jan 2019 to 29 May 2025\n",
      "    Range: 2335 days\n",
      "\n",
      "=== OVERALL DATE RANGE ===\n",
      "\n",
      "Oldest date: 21 Nov 2018 (from inventory_items.created_at)\n",
      "Newest date: 04 Jun 2025 (from orders.returned_at)\n",
      "Total time span: 2386 days\n"
     ]
    }
   ],
   "source": [
    "# Date range analysis\n",
    "print(\"=== DATE RANGES ===\")\n",
    "\n",
    "# Convert date columns to datetime if they aren't already\n",
    "date_columns = {\n",
    "    'users': ['created_at'],\n",
    "    'events': ['created_at'],\n",
    "    'orders': ['created_at', 'returned_at', 'shipped_at', 'delivered_at'],\n",
    "    'order_items': ['created_at', 'shipped_at', 'delivered_at', 'returned_at'],\n",
    "    'inventory_items': ['created_at', 'sold_at']\n",
    "}\n",
    "\n",
    "# Track sources of min/max dates\n",
    "min_date_source = None\n",
    "max_date_source = None\n",
    "\n",
    "def convert_and_range(df, col, table_name):\n",
    "    global overall_min, overall_max, min_date_source, max_date_source\n",
    "    \n",
    "    if col in df.columns:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        valid_dates = df[col].dropna()\n",
    "        if not valid_dates.empty:\n",
    "            min_date, max_date = valid_dates.min(), valid_dates.max()\n",
    "            print(f\"  {col}: {min_date.strftime('%d %b %Y')} to {max_date.strftime('%d %b %Y')}\")\n",
    "            print(f\"    Range: {(max_date - min_date).days} days\")\n",
    "            \n",
    "            # Update overall min date if this is earlier\n",
    "            if overall_min is None or min_date < overall_min:\n",
    "                overall_min = min_date\n",
    "                min_date_source = f\"{table_name}.{col}\"\n",
    "                \n",
    "            # Update overall max date if this is later\n",
    "            if overall_max is None or max_date > overall_max:\n",
    "                overall_max = max_date\n",
    "                max_date_source = f\"{table_name}.{col}\"\n",
    "                \n",
    "            return min_date, max_date\n",
    "        else:\n",
    "            print(f\"  {col}: No valid dates\")\n",
    "    return None, None\n",
    "\n",
    "overall_min, overall_max = None, None\n",
    "for table, cols in date_columns.items():\n",
    "    df = locals()[table]\n",
    "    print(f\"\\n{table.upper()}:\")\n",
    "    for col in cols:\n",
    "        min_date, max_date = convert_and_range(df, col, table)\n",
    "\n",
    "if overall_min and overall_max:\n",
    "    print(\"\\n=== OVERALL DATE RANGE ===\")\n",
    "    print(f\"\\nOldest date: {overall_min.strftime('%d %b %Y')} (from {min_date_source})\")\n",
    "    print(f\"Newest date: {overall_max.strftime('%d %b %Y')} (from {max_date_source})\")\n",
    "    print(f\"Total time span: {(overall_max - overall_min).days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f84f27",
   "metadata": {},
   "source": [
    "The dataset spans from `2018-11-21` (`inventory_items.created_at`) to `2025-06-04` (`orders.returned_at`), covering 2,386 days. Key date columns like `users.created_at`, `events.created_at`, and `orders.created_at` show continuous records from early 2019 to mid-2025, indicating sufficient data for longitudinal analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287f565",
   "metadata": {},
   "source": [
    "## Business and Performance Metrics Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88fa6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KEY BUSINESS METRICS ===\n",
      "Total Users: 100,000\n",
      "Total Orders: 125,278\n",
      "Total Products: 29,120\n",
      "Total Revenue: $10,788,195.71\n",
      "\n",
      "=== PERFORMANCE METRICS ===\n",
      "Users with Orders: 79,989\n",
      "Conversion Rate: 79.99%\n",
      "Average Order Value: $86.11\n",
      "Median Order Value: $55.03\n",
      "Revenue per User: $107.88\n",
      "Revenue per Customer: $134.87\n",
      "\n",
      "=== PERFORMANCE METRICS ===\n",
      "Users with Orders: 79,989\n",
      "Conversion Rate: 79.99%\n",
      "Average Order Value: $86.11\n",
      "Median Order Value: $55.03\n",
      "Revenue per User: $107.88\n",
      "Revenue per Customer: $134.87\n",
      "\n",
      "=== ORDER PATTERNS ===\n",
      "Avg Orders per Customer: 1.57\n",
      "Max Orders by Single Customer: 4\n",
      "% of One-time Customers: 62.25%\n",
      "Repeat Customer Rate: 37.75%\n",
      "Order Return Rate: 9.93%\n",
      "Average Items per Order: 1.45\n",
      "Churn Rate (no order in last 6 months): 65.02%\n",
      "\n",
      "=== ORDER PATTERNS ===\n",
      "Avg Orders per Customer: 1.57\n",
      "Max Orders by Single Customer: 4\n",
      "% of One-time Customers: 62.25%\n",
      "Repeat Customer Rate: 37.75%\n",
      "Order Return Rate: 9.93%\n",
      "Average Items per Order: 1.45\n",
      "Churn Rate (no order in last 6 months): 65.02%\n"
     ]
    }
   ],
   "source": [
    "print(\"=== KEY BUSINESS METRICS ===\")\n",
    "\n",
    "# Basic counts\n",
    "total_users = users['id'].nunique()\n",
    "total_orders = orders['order_id'].nunique()\n",
    "total_products = products['id'].nunique()\n",
    "total_revenue = order_items['sale_price'].sum()\n",
    "\n",
    "print(f\"Total Users: {total_users:,}\")\n",
    "print(f\"Total Orders: {total_orders:,}\")\n",
    "print(f\"Total Products: {total_products:,}\")\n",
    "print(f\"Total Revenue: ${total_revenue:,.2f}\")\n",
    "\n",
    "# Calculate key metrics\n",
    "users_with_orders = orders['user_id'].nunique()\n",
    "conversion_rate = (users_with_orders / total_users) * 100\n",
    "aov = total_revenue / total_orders\n",
    "median_order_value = order_items.groupby('order_id')['sale_price'].sum().median()\n",
    "revenue_per_user = total_revenue / total_users\n",
    "revenue_per_customer = total_revenue / users_with_orders\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE METRICS ===\")\n",
    "print(f\"Users with Orders: {users_with_orders:,}\")\n",
    "print(f\"Conversion Rate: {conversion_rate:.2f}%\")\n",
    "print(f\"Average Order Value: ${aov:.2f}\")\n",
    "print(f\"Median Order Value: ${median_order_value:.2f}\")\n",
    "print(f\"Revenue per User: ${revenue_per_user:.2f}\")\n",
    "print(f\"Revenue per Customer: ${revenue_per_customer:.2f}\")\n",
    "\n",
    "# Order statistics\n",
    "orders_per_customer = orders.groupby('user_id').size()\n",
    "repeat_customers = (orders_per_customer > 1).sum()\n",
    "repeat_customer_rate = (repeat_customers / users_with_orders) * 100\n",
    "returned_orders = orders[orders['status'].str.lower().str.contains('return')]['order_id'].nunique()\n",
    "order_return_rate = (returned_orders / total_orders) * 100 if total_orders else 0\n",
    "items_per_order = order_items.groupby('order_id').size().mean()\n",
    "cutoff_date = (pd.Timestamp(datetime.date.today()) - pd.DateOffset(months=6)).tz_localize('UTC')\n",
    "last_order_dates = orders.groupby('user_id')['created_at'].max()\n",
    "churned_users = (last_order_dates < cutoff_date).sum()\n",
    "churn_rate = (churned_users / users_with_orders) * 100 if users_with_orders else 0\n",
    "print(f\"\\n=== ORDER PATTERNS ===\")\n",
    "print(f\"Avg Orders per Customer: {orders_per_customer.mean():.2f}\")\n",
    "print(f\"Max Orders by Single Customer: {orders_per_customer.max()}\")\n",
    "print(f\"% of One-time Customers: {(orders_per_customer == 1).mean() * 100:.2f}%\")\n",
    "print(f\"Repeat Customer Rate: {repeat_customer_rate:.2f}%\")\n",
    "print(f\"Order Return Rate: {order_return_rate:.2f}%\")\n",
    "print(f\"Average Items per Order: {items_per_order:.2f}\")\n",
    "print(f\"Churn Rate (no order in last 6 months): {churn_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e52e095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time to Ship: 1.50 days\n",
      "Average Time to Deliver: 2.49 days\n",
      "\n",
      "Customer Segmentation:\n",
      "  One-time Customers: 49,795 (49.80%)\n",
      "  Repeat Customers: 30,194 (30.19%)\n",
      "\n",
      "Conversion Rate by Traffic Source:\n",
      "  Display: 79.69% (3189/4002)\n",
      "  Email: 80.37% (3917/4874)\n",
      "  Facebook: 79.90% (4930/6170)\n",
      "  Organic: 80.16% (11883/14824)\n",
      "  Search: 79.95% (56070/70130)\n",
      "\n",
      "Order Status Distribution:\n",
      "  Shipped: 37,660 orders (30.1%)\n",
      "  Complete: 31,614 orders (25.2%)\n",
      "  Processing: 24,845 orders (19.8%)\n",
      "  Cancelled: 18,714 orders (14.9%)\n",
      "  Returned: 12,445 orders (9.9%)\n",
      "\n",
      "Conversion Rate by Traffic Source:\n",
      "  Display: 79.69% (3189/4002)\n",
      "  Email: 80.37% (3917/4874)\n",
      "  Facebook: 79.90% (4930/6170)\n",
      "  Organic: 80.16% (11883/14824)\n",
      "  Search: 79.95% (56070/70130)\n",
      "\n",
      "Order Status Distribution:\n",
      "  Shipped: 37,660 orders (30.1%)\n",
      "  Complete: 31,614 orders (25.2%)\n",
      "  Processing: 24,845 orders (19.8%)\n",
      "  Cancelled: 18,714 orders (14.9%)\n",
      "  Returned: 12,445 orders (9.9%)\n"
     ]
    }
   ],
   "source": [
    "# Other performance metrics\n",
    "\n",
    "# Average Shipping and Delivery Time (in days)\n",
    "orders_shipped = orders[orders['shipped_at'].notna() & orders['created_at'].notna()]\n",
    "orders_delivered = orders[orders['delivered_at'].notna() & orders['shipped_at'].notna()]\n",
    "if not orders_shipped.empty:\n",
    "    avg_ship_time = (orders_shipped['shipped_at'] - orders_shipped['created_at']).dt.total_seconds().mean() / 86400\n",
    "    print(f\"Average Time to Ship: {avg_ship_time:.2f} days\")\n",
    "else:\n",
    "    print(\"\\nAverage Time to Ship: N/A\")\n",
    "if not orders_delivered.empty:\n",
    "    avg_delivery_time = (orders_delivered['delivered_at'] - orders_delivered['shipped_at']).dt.total_seconds().mean() / 86400\n",
    "    print(f\"Average Time to Deliver: {avg_delivery_time:.2f} days\")\n",
    "else:\n",
    "    print(\"Average Time to Deliver: N/A\")\n",
    "    \n",
    "# Customer Segmentation: Repeat vs. One-time Customers\n",
    "one_time = (orders_per_customer == 1).sum()\n",
    "repeat = (orders_per_customer > 1).sum()\n",
    "print(f\"\\nCustomer Segmentation:\")\n",
    "print(f\"  One-time Customers: {one_time:,} ({(one_time/total_users)*100:.2f}%)\")\n",
    "print(f\"  Repeat Customers: {repeat:,} ({(repeat/total_users)*100:.2f}%)\")\n",
    "    \n",
    "# Conversion Rate by Traffic Source\n",
    "traffic_conv = users.merge(orders[['user_id']], left_on='id', right_on='user_id', how='left', indicator=True)\n",
    "traffic_conv['is_buyer'] = traffic_conv['_merge'] == 'both'\n",
    "traffic_conv_deduped = traffic_conv.drop_duplicates('id')\n",
    "conv_by_source = traffic_conv_deduped.groupby('traffic_source').agg(\n",
    "    total_users=('id','count'),\n",
    "    buyers=('is_buyer','sum')\n",
    ")\n",
    "conv_by_source['conversion_rate'] = (conv_by_source['buyers'] / conv_by_source['total_users']) * 100\n",
    "print(\"\\nConversion Rate by Traffic Source:\")\n",
    "for idx, row in conv_by_source.iterrows():\n",
    "    print(f\"  {idx}: {row['conversion_rate']:.2f}% ({int(row['buyers'])}/{int(row['total_users'])})\")\n",
    "    \n",
    "# Order status distribution\n",
    "status_counts = orders['status'].value_counts()\n",
    "print(\"\\nOrder Status Distribution:\")\n",
    "for status, count in status_counts.items():\n",
    "    pct = (count / total_orders) * 100\n",
    "    print(f\"  {status}: {count:,} orders ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10ea78",
   "metadata": {},
   "source": [
    "## Top Customers, Categories, Brands, and Products by Revenue and Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51b4496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Customers by Lifetime Value (CLV):\n",
      "  Charles Spencer: $2,015.92\n",
      "  Devin White: $1,626.13\n",
      "  Robert Gonzalez: $1,535.02\n",
      "  Joseph Stewart: $1,520.93\n",
      "  Jennifer Ramos: $1,490.99\n",
      "\n",
      "Top 5 Product Categories by Revenue:\n",
      "  Outerwear & Coats: $1,291,409.80\n",
      "  Jeans: $1,255,639.76\n",
      "  Sweaters: $830,785.40\n",
      "  Swim: $647,627.43\n",
      "  Suits & Sport Coats: $647,153.19\n",
      "\n",
      "Top 5 Product Categories by Order Count:\n",
      "  Intimates: 12,713 orders\n",
      "  Jeans: 12,434 orders\n",
      "  Tops & Tees: 11,564 orders\n",
      "  Fashion Hoodies & Sweatshirts: 11,344 orders\n",
      "  Swim: 11,076 orders\n",
      "\n",
      "Top 5 Brands by Revenue:\n",
      "  Diesel: $201,557.41\n",
      "  Calvin Klein: $201,503.74\n",
      "  True Religion: $179,670.11\n",
      "  7 For All Mankind: $172,409.43\n",
      "  Carhartt: $169,504.77\n",
      "\n",
      "Top 5 Brands by Order Count:\n",
      "  Allegra K: 6,130 orders\n",
      "  Calvin Klein: 3,112 orders\n",
      "  Carhartt: 2,506 orders\n",
      "  Hanes: 1,956 orders\n",
      "  Volcom: 1,850 orders\n",
      "\n",
      "Top 5 Products by Revenue:\n",
      "  The North Face Apex Bionic Soft Shell Jacket - Men's (Price: $903.00): $11,739.00\n",
      "  ASCIS Cushion Low Socks (Pack of 3) (Price: $903.00): $9,933.00\n",
      "  The North Face Apex Bionic Jacket - Men's (Price: $903.00): $8,127.00\n",
      "  The North Face Apex Bionic Soft Shell Jacket - Men's (Price: $903.00): $7,224.00\n",
      "  JORDAN DURASHEEN SHORT MENS 404309-109 (Price: $903.00): $7,224.00\n",
      "\n",
      "Top 5 Products by Order Amount:\n",
      "  G-Star Men's Cm Rovic Arc 3D Loose Tapered Pant (Price: $95.00): 20 orders\n",
      "  Woolrich Men's Elite Waterproof Breathable Tactical Parka Jacket (Price: $180.85): 19 orders\n",
      "  Volcom Men's Maguro Block Boardshort (Price: $49.50): 18 orders\n",
      "  Oakley Originate Fleece Hoodie 2013 (Price: $99.95): 18 orders\n",
      "  Shapewear Smoothing Slimming Control Bodysuit (Price: $39.95): 17 orders\n",
      "\n",
      "Top 5 Product Categories by Revenue:\n",
      "  Outerwear & Coats: $1,291,409.80\n",
      "  Jeans: $1,255,639.76\n",
      "  Sweaters: $830,785.40\n",
      "  Swim: $647,627.43\n",
      "  Suits & Sport Coats: $647,153.19\n",
      "\n",
      "Top 5 Product Categories by Order Count:\n",
      "  Intimates: 12,713 orders\n",
      "  Jeans: 12,434 orders\n",
      "  Tops & Tees: 11,564 orders\n",
      "  Fashion Hoodies & Sweatshirts: 11,344 orders\n",
      "  Swim: 11,076 orders\n",
      "\n",
      "Top 5 Brands by Revenue:\n",
      "  Diesel: $201,557.41\n",
      "  Calvin Klein: $201,503.74\n",
      "  True Religion: $179,670.11\n",
      "  7 For All Mankind: $172,409.43\n",
      "  Carhartt: $169,504.77\n",
      "\n",
      "Top 5 Brands by Order Count:\n",
      "  Allegra K: 6,130 orders\n",
      "  Calvin Klein: 3,112 orders\n",
      "  Carhartt: 2,506 orders\n",
      "  Hanes: 1,956 orders\n",
      "  Volcom: 1,850 orders\n",
      "\n",
      "Top 5 Products by Revenue:\n",
      "  The North Face Apex Bionic Soft Shell Jacket - Men's (Price: $903.00): $11,739.00\n",
      "  ASCIS Cushion Low Socks (Pack of 3) (Price: $903.00): $9,933.00\n",
      "  The North Face Apex Bionic Jacket - Men's (Price: $903.00): $8,127.00\n",
      "  The North Face Apex Bionic Soft Shell Jacket - Men's (Price: $903.00): $7,224.00\n",
      "  JORDAN DURASHEEN SHORT MENS 404309-109 (Price: $903.00): $7,224.00\n",
      "\n",
      "Top 5 Products by Order Amount:\n",
      "  G-Star Men's Cm Rovic Arc 3D Loose Tapered Pant (Price: $95.00): 20 orders\n",
      "  Woolrich Men's Elite Waterproof Breathable Tactical Parka Jacket (Price: $180.85): 19 orders\n",
      "  Volcom Men's Maguro Block Boardshort (Price: $49.50): 18 orders\n",
      "  Oakley Originate Fleece Hoodie 2013 (Price: $99.95): 18 orders\n",
      "  Shapewear Smoothing Slimming Control Bodysuit (Price: $39.95): 17 orders\n"
     ]
    }
   ],
   "source": [
    "# Top 5 Customer Lifetime Value (CLV)\n",
    "customer_revenue = orders.merge(order_items, left_on='order_id', right_on='order_id')\n",
    "grouped_customers = customer_revenue.groupby('user_id_x')['sale_price'].sum().sort_values(ascending=False)\n",
    "top_customers = users.set_index('id').loc[grouped_customers.head(5).index]\n",
    "top_customers = top_customers[['first_name', 'last_name']].copy()\n",
    "top_customers['revenue'] = grouped_customers.head(5).values\n",
    "print(\"Top 5 Customers by Lifetime Value (CLV):\")\n",
    "for i, row in top_customers.iterrows():\n",
    "    print(f\"  {row['first_name']} {row['last_name']}: ${row['revenue']:,.2f}\")\n",
    "\n",
    "# Top 5 category by revenue\n",
    "category_revenue = order_items.merge(products[['id','category']], left_on='product_id', right_on='id')\n",
    "category_revenue = category_revenue.groupby('category')['sale_price'].sum().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 Product Categories by Revenue:\")\n",
    "for cat, val in category_revenue.head(5).items():\n",
    "    print(f\"  {cat}: ${val:,.2f}\")\n",
    "\n",
    "# Top 5 category by order count (popularity)\n",
    "category_popularity = order_items.merge(products[['id','category']], left_on='product_id', right_on='id')\n",
    "category_order_count = category_popularity.groupby('category')['order_id'].nunique().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 Product Categories by Order Count:\")\n",
    "for cat, count in category_order_count.head(5).items():\n",
    "    print(f\"  {cat}: {count:,} orders\")\n",
    "\n",
    "# Top 5 brand by revenue\n",
    "brand_revenue = order_items.merge(products[['id','brand']], left_on='product_id', right_on='id')\n",
    "brand_revenue = brand_revenue.groupby('brand')['sale_price'].sum().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 Brands by Revenue:\")\n",
    "for brand, val in brand_revenue.head(5).items():\n",
    "    print(f\"  {brand}: ${val:,.2f}\")\n",
    "\n",
    "# Top 5 brand by order count (popularity)\n",
    "brand_popularity = order_items.merge(products[['id','brand']], left_on='product_id', right_on='id')\n",
    "brand_order_count = brand_popularity.groupby('brand')['order_id'].nunique().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 Brands by Order Count:\")\n",
    "for brand, count in brand_order_count.head(5).items():\n",
    "    print(f\"  {brand}: {count:,} orders\")\n",
    "\n",
    "# Top 5 products by revenue\n",
    "product_revenue = order_items.groupby('product_id')['sale_price'].sum().sort_values(ascending=False)\n",
    "top_products = products.set_index('id').loc[product_revenue.head(5).index]\n",
    "top_products = top_products[['name', 'retail_price']].copy()\n",
    "top_products['revenue'] = product_revenue.head(5).values\n",
    "print(\"\\nTop 5 Products by Revenue:\")\n",
    "for i, row in top_products.iterrows():\n",
    "    print(f\"  {row['name']} (Price: ${row['retail_price']:.2f}): ${row['revenue']:,.2f}\")\n",
    "\n",
    "# Top 5 products by order count (popularity)\n",
    "product_order_count = order_items['product_id'].value_counts().head(5)\n",
    "top_products_count = products.set_index('id').loc[product_order_count.index]\n",
    "top_products_count = top_products_count[['name', 'retail_price']].copy()\n",
    "top_products_count['order_count'] = product_order_count.values\n",
    "print(\"\\nTop 5 Products by Order Amount:\")\n",
    "for i, row in top_products_count.iterrows():\n",
    "    print(f\"  {row['name']} (Price: ${row['retail_price']:.2f}): {row['order_count']:,} orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f49503",
   "metadata": {},
   "source": [
    "## Customer Demography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bd78fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CUSTOMER DEMOGRAPHY ===\n",
      "\n",
      "Gender Distribution:\n",
      "  F: 50,172 (50.17%)\n",
      "  M: 49,828 (49.83%)\n",
      "\n",
      "Age Statistics:\n",
      "  Mean Age: 41.0\n",
      "  Median Age: 41.0\n",
      "  Min Age: 12\n",
      "  Max Age: 70\n",
      "  Age Groups:\n",
      "    <18: 10,195 (10.20%)\n",
      "    18-24: 11,756 (11.76%)\n",
      "    25-34: 16,976 (16.98%)\n",
      "    35-44: 16,828 (16.83%)\n",
      "    45-54: 16,980 (16.98%)\n",
      "    55-64: 17,152 (17.15%)\n",
      "    65+: 10,113 (10.11%)\n",
      "\n",
      "Top 5 Countrys by User Count:\n",
      "  China: 33,821\n",
      "  United States: 22,516\n",
      "  Brasil: 14,511\n",
      "  South Korea: 5,304\n",
      "  United Kingdom: 4,711\n",
      "\n",
      "Top 5 States by User Count:\n",
      "  Guangdong: 5,425\n",
      "  England: 4,170\n",
      "  California: 3,745\n",
      "  Texas: 2,513\n",
      "  Shanghai: 2,423\n",
      "\n",
      "Top 5 Citys by User Count:\n",
      "  Shanghai: 2,483\n",
      "  Beijing: 2,042\n",
      "  Seoul: 1,444\n",
      "  Shenzhen: 1,292\n",
      "  null: 992\n",
      "\n",
      "Traffic Source Distribution:\n",
      "  Search: 70,130 (70.13%)\n",
      "  Organic: 14,824 (14.82%)\n",
      "  Facebook: 6,170 (6.17%)\n",
      "  Email: 4,874 (4.87%)\n",
      "  Display: 4,002 (4.00%)\n",
      "\n",
      "Customer Tenure (days since registration):\n",
      "  Mean: 1144.6\n",
      "  Median: 1143.0\n",
      "  Min: 7\n",
      "  Max: 2341\n",
      "\n",
      "Average Orders per User by Gender and Age Group:\n",
      "age_group   <18  18-24  25-34  35-44  45-54  55-64   65+\n",
      "gender                                                  \n",
      "F          1.23   1.24   1.26   1.25   1.24   1.28  1.25\n",
      "M          1.25   1.26   1.26   1.26   1.24   1.24  1.26\n",
      "\n",
      "Top 5 Cities by Order Count:\n",
      "  Shanghai: 3,187 orders\n",
      "  Beijing: 2,428 orders\n",
      "  Seoul: 1,831 orders\n",
      "  Shenzhen: 1,641 orders\n",
      "  Dongguan: 1,240 orders\n"
     ]
    }
   ],
   "source": [
    "# --- Customer Demography EDA ---\n",
    "print(\"=== CUSTOMER DEMOGRAPHY ===\")\n",
    "\n",
    "# Gender distribution\n",
    "if 'gender' in users.columns:\n",
    "    gender_counts = users['gender'].value_counts(dropna=False)\n",
    "    gender_pct = users['gender'].value_counts(normalize=True, dropna=False) * 100\n",
    "    print(\"\\nGender Distribution:\")\n",
    "    for g, c in gender_counts.items():\n",
    "        print(f\"  {g}: {c:,} ({gender_pct[g]:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo gender data available.\")\n",
    "\n",
    "# Age distribution\n",
    "if 'age' in users.columns:\n",
    "    print(\"\\nAge Statistics:\")\n",
    "    print(f\"  Mean Age: {users['age'].mean():.1f}\")\n",
    "    print(f\"  Median Age: {users['age'].median():.1f}\")\n",
    "    print(f\"  Min Age: {users['age'].min()}\")\n",
    "    print(f\"  Max Age: {users['age'].max()}\")\n",
    "    print(\"  Age Groups:\")\n",
    "    age_bins = [0, 18, 25, 35, 45, 55, 65, 100]\n",
    "    age_labels = ['<18', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "    users['age_group'] = pd.cut(users['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "    age_group_counts = users['age_group'].value_counts(sort=False)\n",
    "    for group, count in age_group_counts.items():\n",
    "        pct = (count / len(users)) * 100\n",
    "        print(f\"    {group}: {count:,} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo age data available.\")\n",
    "\n",
    "# Geographic distribution (country, state, city)\n",
    "for col in ['country', 'state', 'city']:\n",
    "    if col in users.columns:\n",
    "        top_geo = users[col].value_counts().head(5)\n",
    "        print(f\"\\nTop 5 {col.title()}s by User Count:\")\n",
    "        for val, cnt in top_geo.items():\n",
    "            print(f\"  {val}: {cnt:,}\")\n",
    "    else:\n",
    "        print(f\"\\nNo {col} data available.\")\n",
    "\n",
    "# Traffic source breakdown\n",
    "if 'traffic_source' in users.columns:\n",
    "    traffic_counts = users['traffic_source'].value_counts()\n",
    "    traffic_pct = users['traffic_source'].value_counts(normalize=True) * 100\n",
    "    print(\"\\nTraffic Source Distribution:\")\n",
    "    for src, cnt in traffic_counts.items():\n",
    "        print(f\"  {src}: {cnt:,} ({traffic_pct[src]:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo traffic source data available.\")\n",
    "\n",
    "# Customer tenure (how long since registration)\n",
    "if 'created_at' in users.columns:\n",
    "    users['created_at'] = pd.to_datetime(users['created_at'], errors='coerce')\n",
    "    # Make the current timestamp timezone-aware with UTC to match users['created_at']\n",
    "    today = pd.Timestamp(datetime.date.today(), tz='UTC')\n",
    "    users['tenure_days'] = (today - users['created_at']).dt.days\n",
    "    print(\"\\nCustomer Tenure (days since registration):\")\n",
    "    print(f\"  Mean: {users['tenure_days'].mean():.1f}\")\n",
    "    print(f\"  Median: {users['tenure_days'].median():.1f}\")\n",
    "    print(f\"  Min: {users['tenure_days'].min()}\")\n",
    "    print(f\"  Max: {users['tenure_days'].max()}\")\n",
    "else:\n",
    "    print(\"\\nNo registration date data available.\")\n",
    "\n",
    "# Customer segmentation by demography and purchasing\n",
    "if 'gender' in users.columns and 'age_group' in users.columns:\n",
    "    # Merge with orders to get purchasing info\n",
    "    user_orders = orders.groupby('user_id').size().rename('order_count')\n",
    "    users_demo = users.join(user_orders, how='left').fillna({'order_count': 0})\n",
    "    print(\"\\nAverage Orders per User by Gender and Age Group:\")\n",
    "    seg = users_demo.groupby(['gender', 'age_group'], observed=True)['order_count'].mean().unstack()\n",
    "    print(seg.round(2))\n",
    "else:\n",
    "    print(\"\\nNot enough demographic data for segmentation.\")\n",
    "\n",
    "# Top cities/states/countries by revenue (if location and orders can be joined)\n",
    "if 'city' in users.columns:\n",
    "    user_revenue = orders.merge(users[['id', 'city', 'state', 'country']], left_on='user_id', right_on='id', how='left')\n",
    "    city_rev = user_revenue.groupby('city')['order_id'].count().sort_values(ascending=False).head(5)\n",
    "    print(\"\\nTop 5 Cities by Order Count:\")\n",
    "    for city, cnt in city_rev.items():\n",
    "        print(f\"  {city}: {cnt:,} orders\")\n",
    "else:\n",
    "    print(\"\\nNo city data for revenue analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
