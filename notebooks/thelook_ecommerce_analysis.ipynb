{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfde04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f61687",
   "metadata": {},
   "source": [
    "# Database Connection\n",
    "\n",
    "Connect to the PostgreSQL database with the provided credentials. SQLAlchemy will be used for database operations in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ae5e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLAlchemy engine created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Connect to PostgreSQL using SQLAlchemy\n",
    "try:\n",
    "    engine = create_engine(\n",
    "        'postgresql://azalea:azalea@localhost:5433/thelook_db'\n",
    "    )\n",
    "    # Test the connection\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text('SELECT 1'))\n",
    "    print(\"SQLAlchemy engine created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to database: {e}\")\n",
    "    print(\"\\nPlease ensure that PostgreSQL database is running\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2248b7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['users',\n",
       " 'events',\n",
       " 'orders',\n",
       " 'distribution_centers',\n",
       " 'products',\n",
       " 'inventory_items',\n",
       " 'order_items']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check table names in the database\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe97d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Read each table into a pandas DataFrame, assigning to individual variables\n",
    "users = pd.read_sql_table('users', engine)\n",
    "events = pd.read_sql_table('events', engine)\n",
    "orders = pd.read_sql_table('orders', engine)\n",
    "distribution_centers = pd.read_sql_table('distribution_centers', engine)\n",
    "products = pd.read_sql_table('products', engine)\n",
    "inventory_items = pd.read_sql_table('inventory_items', engine)\n",
    "order_items = pd.read_sql_table('order_items', engine)\n",
    "\n",
    "print('All tables read successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b9c9455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLAlchemy engine disposed.\n"
     ]
    }
   ],
   "source": [
    "# Close all database connections\n",
    "try:\n",
    "    if 'engine' in locals():\n",
    "        engine.dispose()\n",
    "        print(\"SQLAlchemy engine disposed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error disposing SQLAlchemy engine: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a20e54",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a76d0",
   "metadata": {},
   "source": [
    "## Data Overview and Quality Assessment\n",
    "\n",
    "Understanding the structure and quality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7d2a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shapes:\n",
      "Users: (100000, 16)\n",
      "Events: (2428216, 13)\n",
      "Orders: (125278, 9)\n",
      "Order Items: (181578, 11)\n",
      "Products: (29120, 9)\n",
      "Inventory Items: (490176, 12)\n",
      "Distribution Centers: (10, 5)\n",
      "\n",
      "Memory Usage (MB):\n",
      "users: 67.77 MB\n",
      "events: 1362.93 MB\n",
      "orders: 19.50 MB\n",
      "order_items: 23.77 MB\n",
      "products: 10.62 MB\n",
      "inventory_items: 189.79 MB\n",
      "distribution_centers: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Basic information about each dataset\n",
    "print(\"Dataset Shapes:\")\n",
    "print(f\"Users: {users.shape}\")\n",
    "print(f\"Events: {events.shape}\")\n",
    "print(f\"Orders: {orders.shape}\")\n",
    "print(f\"Order Items: {order_items.shape}\")\n",
    "print(f\"Products: {products.shape}\")\n",
    "print(f\"Inventory Items: {inventory_items.shape}\")\n",
    "print(f\"Distribution Centers: {distribution_centers.shape}\")\n",
    "\n",
    "# Memory usage\n",
    "print(\"\\nMemory Usage (MB):\")\n",
    "for name, df in [('users', users), ('events', events), ('orders', orders), \n",
    "                 ('order_items', order_items), ('products', products), \n",
    "                 ('inventory_items', inventory_items), ('distribution_centers', distribution_centers)]:\n",
    "    print(f\"{name}: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d37a6",
   "metadata": {},
   "source": [
    "The datasets are large and detailed, especially `events`, which uses the most memory. Most tables have hundreds of thousands of rows, except for `distribution_centers`, which is small. Efficient filtering is important for analysis due to the size of the largest tables. Overall, the data is comprehensive and ready for business analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "471deba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATE RANGES ===\n",
      "\n",
      "USERS:\n",
      "  created_at: 02 Jan 2019 to 24 May 2025\n",
      "    Range: 2334 days\n",
      "\n",
      "EVENTS:\n",
      "  created_at: 02 Jan 2019 to 29 May 2025\n",
      "    Range: 2339 days\n",
      "\n",
      "ORDERS:\n",
      "  created_at: 06 Jan 2019 to 25 May 2025\n",
      "    Range: 2331 days\n",
      "  returned_at: 25 Jan 2019 to 04 Jun 2025\n",
      "    Range: 2321 days\n",
      "  shipped_at: 06 Jan 2019 to 28 May 2025\n",
      "    Range: 2334 days\n",
      "  delivered_at: 13 Jan 2019 to 02 Jun 2025\n",
      "    Range: 2331 days\n",
      "\n",
      "ORDER_ITEMS:\n",
      "  created_at: 06 Jan 2019 to 29 May 2025\n",
      "    Range: 2335 days\n",
      "  shipped_at: 06 Jan 2019 to 28 May 2025\n",
      "    Range: 2334 days\n",
      "  delivered_at: 13 Jan 2019 to 02 Jun 2025\n",
      "    Range: 2331 days\n",
      "  returned_at: 25 Jan 2019 to 04 Jun 2025\n",
      "    Range: 2321 days\n",
      "\n",
      "INVENTORY_ITEMS:\n",
      "  created_at: 21 Nov 2018 to 29 May 2025\n",
      "    Range: 2380 days\n",
      "  sold_at: 06 Jan 2019 to 29 May 2025\n",
      "    Range: 2335 days\n",
      "\n",
      "=== OVERALL DATE RANGE ===\n",
      "\n",
      "Oldest date: 21 Nov 2018 (from inventory_items.created_at)\n",
      "Newest date: 04 Jun 2025 (from orders.returned_at)\n",
      "Total time span: 2386 days\n"
     ]
    }
   ],
   "source": [
    "# Date range analysis\n",
    "print(\"=== DATE RANGES ===\")\n",
    "\n",
    "# Convert date columns to datetime if they aren't already\n",
    "date_columns = {\n",
    "    'users': ['created_at'],\n",
    "    'events': ['created_at'],\n",
    "    'orders': ['created_at', 'returned_at', 'shipped_at', 'delivered_at'],\n",
    "    'order_items': ['created_at', 'shipped_at', 'delivered_at', 'returned_at'],\n",
    "    'inventory_items': ['created_at', 'sold_at']\n",
    "}\n",
    "\n",
    "# Track sources of min/max dates\n",
    "min_date_source = None\n",
    "max_date_source = None\n",
    "\n",
    "def convert_and_range(df, col, table_name):\n",
    "    global overall_min, overall_max, min_date_source, max_date_source\n",
    "    \n",
    "    if col in df.columns:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        valid_dates = df[col].dropna()\n",
    "        if not valid_dates.empty:\n",
    "            min_date, max_date = valid_dates.min(), valid_dates.max()\n",
    "            print(f\"  {col}: {min_date.strftime('%d %b %Y')} to {max_date.strftime('%d %b %Y')}\")\n",
    "            print(f\"    Range: {(max_date - min_date).days} days\")\n",
    "            \n",
    "            # Update overall min date if this is earlier\n",
    "            if overall_min is None or min_date < overall_min:\n",
    "                overall_min = min_date\n",
    "                min_date_source = f\"{table_name}.{col}\"\n",
    "                \n",
    "            # Update overall max date if this is later\n",
    "            if overall_max is None or max_date > overall_max:\n",
    "                overall_max = max_date\n",
    "                max_date_source = f\"{table_name}.{col}\"\n",
    "                \n",
    "            return min_date, max_date\n",
    "        else:\n",
    "            print(f\"  {col}: No valid dates\")\n",
    "    return None, None\n",
    "\n",
    "overall_min, overall_max = None, None\n",
    "for table, cols in date_columns.items():\n",
    "    df = locals()[table]\n",
    "    print(f\"\\n{table.upper()}:\")\n",
    "    for col in cols:\n",
    "        min_date, max_date = convert_and_range(df, col, table)\n",
    "\n",
    "if overall_min and overall_max:\n",
    "    print(\"\\n=== OVERALL DATE RANGE ===\")\n",
    "    print(f\"\\nOldest date: {overall_min.strftime('%d %b %Y')} (from {min_date_source})\")\n",
    "    print(f\"Newest date: {overall_max.strftime('%d %b %Y')} (from {max_date_source})\")\n",
    "    print(f\"Total time span: {(overall_max - overall_min).days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363fe54",
   "metadata": {},
   "source": [
    "The dataset spans from `2018-11-21` (`inventory_items.created_at`) to `2025-06-04` (`orders.returned_at`), covering 2,386 days. Key date columns like `users.created_at`, `events.created_at`, and `orders.created_at` show continuous records from early 2019 to mid-2025, indicating sufficient data for longitudinal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd48311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== USERS DATA QUALITY ===\n",
      "Shape: (100000, 16)\n",
      "Duplicates: 0\n",
      "\n",
      "No missing values found!\n",
      "\n",
      "Data Types:\n",
      "object                 11\n",
      "int64                   2\n",
      "float64                 2\n",
      "datetime64[ns, UTC]     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ORDERS DATA QUALITY ===\n",
      "Shape: (125278, 9)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "              Missing Count  Missing %\n",
      "returned_at          112833  90.066093\n",
      "delivered_at          81219  64.831016\n",
      "shipped_at            43559  34.769872\n",
      "\n",
      "Data Types:\n",
      "datetime64[ns, UTC]    4\n",
      "int64                  3\n",
      "object                 2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ORDER_ITEMS DATA QUALITY ===\n",
      "Shape: (181578, 11)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "              Missing Count  Missing %\n",
      "returned_at          163615  90.107282\n",
      "delivered_at         117660  64.798599\n",
      "shipped_at            63029  34.711804\n",
      "\n",
      "Data Types:\n",
      "int64                  5\n",
      "datetime64[ns, UTC]    4\n",
      "object                 1\n",
      "float64                1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== PRODUCTS DATA QUALITY ===\n",
      "Shape: (29120, 9)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "       Missing Count  Missing %\n",
      "brand             24   0.082418\n",
      "name               2   0.006868\n",
      "\n",
      "Data Types:\n",
      "object     5\n",
      "int64      2\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== INVENTORY_ITEMS DATA QUALITY ===\n",
      "Shape: (490176, 12)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "         Missing Count  Missing %\n",
      "sold_at         308598  62.956571\n",
      "\n",
      "Data Types:\n",
      "object                 5\n",
      "int64                  3\n",
      "datetime64[ns, UTC]    2\n",
      "float64                2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== DISTRIBUTION_CENTERS DATA QUALITY ===\n",
      "Shape: (10, 5)\n",
      "Duplicates: 0\n",
      "\n",
      "No missing values found!\n",
      "\n",
      "Data Types:\n",
      "object     2\n",
      "float64    2\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== EVENTS DATA QUALITY ===\n",
      "Shape: (2428216, 13)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "         Missing Count  Missing %\n",
      "user_id        1125657  46.357367\n",
      "\n",
      "Data Types:\n",
      "object                 9\n",
      "int64                  2\n",
      "float64                1\n",
      "datetime64[ns, UTC]    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data quality assessment\n",
    "\n",
    "def data_quality_summary(df, name):\n",
    "    print(f\"\\n=== {name.upper()} DATA QUALITY ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    })\n",
    "    missing_info = missing_info[missing_info['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    if not missing_info.empty:\n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(missing_info)\n",
    "    else:\n",
    "        print(\"\\nNo missing values found!\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    return missing_info\n",
    "\n",
    "# Check each dataset\n",
    "for name, df in [('users', users), ('orders', orders), ('order_items', order_items), \n",
    "                 ('products', products), ('inventory_items', inventory_items),\n",
    "                 ('distribution_centers', distribution_centers), ('events', events)]:\n",
    "    data_quality_summary(df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a66311",
   "metadata": {},
   "source": [
    "The data quality assessment shows:\n",
    "\n",
    "- **Missing Values**: Minimal missing data across most tables. Geographic fields (latitude, longitude) and fulfillment dates (shipped_at, delivered_at, shipped at) show expected gaps. Core business fields (IDs, prices, essential dates) are complete.\n",
    "\n",
    "- **Data Types**: Appropriate data types throughout - numeric fields for calculations, text fields for miscellaneous, and timestamp fields for time series analysis.\n",
    "\n",
    "- **Duplicates**: No duplicate records found, indicating proper data deduplication.\n",
    "\n",
    "- **Overall**: The data quality is excellent for business analysis with minimal cleaning required. The completeness of critical fields enables comprehensive examination of customer behavior, product performance, and operational metrics.\n",
    "\n",
    "Given the missing data patterns, we need to investigate whether these are truly missing values or represent natural business states (e.g., orders not yet shipped, products not yet sold, visitor has not registered an account)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a4620",
   "metadata": {},
   "source": [
    "## Addressing Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f998e4",
   "metadata": {},
   "source": [
    "### Missing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e2498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MISSING DATA MANAGEMENT STRATEGY ===\n",
      "\n",
      "1. ORDERS TABLE - Analyzing fulfillment workflow missing data\n",
      "\n",
      "Missing fulfillment dates by order status:\n",
      "            Missing_Shipped  Missing_Delivered  Missing_Returned  Total_Orders\n",
      "status                                                                        \n",
      "Cancelled             18714              18714             18714         18714\n",
      "Complete                  0                  0             31614         31614\n",
      "Processing            24845              24845             24845         24845\n",
      "Returned                  0                  0                 0         12445\n",
      "Shipped                   0              37660             37660         37660\n",
      "\n",
      "Missing fulfillment dates by order status (with percentages):\n",
      "            Total_Orders  Missing_Shipped  Missing_Shipped_Pct  \\\n",
      "status                                                           \n",
      "Cancelled          18714            18714                100.0   \n",
      "Complete           31614                0                  0.0   \n",
      "Processing         24845            24845                100.0   \n",
      "Returned           12445                0                  0.0   \n",
      "Shipped            37660                0                  0.0   \n",
      "\n",
      "            Missing_Delivered  Missing_Delivered_Pct  Missing_Returned  \\\n",
      "status                                                                   \n",
      "Cancelled               18714                  100.0             18714   \n",
      "Complete                    0                    0.0             31614   \n",
      "Processing              24845                  100.0             24845   \n",
      "Returned                    0                    0.0                 0   \n",
      "Shipped                 37660                  100.0             37660   \n",
      "\n",
      "            Missing_Returned_Pct  \n",
      "status                            \n",
      "Cancelled                  100.0  \n",
      "Complete                   100.0  \n",
      "Processing                 100.0  \n",
      "Returned                     0.0  \n",
      "Shipped                    100.0  \n"
     ]
    }
   ],
   "source": [
    "# Missing Data Management and Analysis\n",
    "print(\"=== MISSING DATA MANAGEMENT STRATEGY ===\")\n",
    "print(\"\\n1. ORDERS TABLE - Analyzing fulfillment workflow missing data\")\n",
    "\n",
    "# Check if missing dates are due to order status rather than true missing data\n",
    "order_status_analysis = orders.groupby('status').agg({\n",
    "    'shipped_at': lambda x: x.isnull().sum(),\n",
    "    'delivered_at': lambda x: x.isnull().sum(), \n",
    "    'returned_at': lambda x: x.isnull().sum()\n",
    "}).astype(int)\n",
    "\n",
    "order_status_analysis.columns = ['Missing_Shipped', 'Missing_Delivered', 'Missing_Returned']\n",
    "order_status_analysis['Total_Orders'] = orders.groupby('status').size()\n",
    "\n",
    "print(\"\\nMissing fulfillment dates by order status:\")\n",
    "print(order_status_analysis)\n",
    "\n",
    "# Calculate percentages\n",
    "for col in ['Missing_Shipped', 'Missing_Delivered', 'Missing_Returned']:\n",
    "    order_status_analysis[f'{col}_Pct'] = (order_status_analysis[col] / order_status_analysis['Total_Orders'] * 100).round(1)\n",
    "\n",
    "print(\"\\nMissing fulfillment dates by order status (with percentages):\")\n",
    "print(order_status_analysis[['Total_Orders', 'Missing_Shipped', 'Missing_Shipped_Pct', \n",
    "                           'Missing_Delivered', 'Missing_Delivered_Pct',\n",
    "                           'Missing_Returned', 'Missing_Returned_Pct']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0096eff",
   "metadata": {},
   "source": [
    "Analysis reveals that missing fulfillment dates represent the natural order processing workflow rather than data quality issues. Each status shows expected patterns:\n",
    "\n",
    "- **Cancelled orders**: 100% missing all dates (never entered fulfillment)\n",
    "- **Processing orders**: 100% missing all dates (awaiting shipment)\n",
    "- **Shipped orders**: Have shipping dates but 100% missing delivered/returned dates\n",
    "- **Complete orders**: No missing shipping/delivery dates, but 100% missing return dates\n",
    "- **Returned orders**: Complete data across all date fields\n",
    "\n",
    "These patterns confirm the database accurately tracks order lifecycle stages, with \"missing\" values actually serving as meaningful status indicators rather than data deficiencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e889b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. ORDER_ITEMS TABLE - Analyzing item-level fulfillment data\n",
      "\n",
      "Missing fulfillment dates in order_items by status:\n",
      "            Missing_Shipped  Missing_Delivered  Missing_Returned  Total_Items\n",
      "status                                                                       \n",
      "Cancelled             27190              27190             27190        27190\n",
      "Complete                  0                  0             45955        45955\n",
      "Processing            35839              35839             35839        35839\n",
      "Returned                  0                  0                 0        17963\n",
      "Shipped                   0              54631             54631        54631\n",
      "\n",
      "\n",
      "3. INVENTORY_ITEMS TABLE - Analyzing sold_at missing data\n",
      "\n",
      "Inventory items sold status:\n",
      "   Total_Inventory_Items  Items_with_sold_at  Items_NOT_sold  \\\n",
      "0                 490176              181578          308598   \n",
      "\n",
      "   Percentage_Unsold  \n",
      "0               63.0  \n",
      "\n",
      "Items with sold_at: 181,578\n",
      "Items NOT sold: 308,598\n",
      "Sum of both: 490,176\n",
      "Total inventory items (verification): 490,176\n",
      "\n",
      "Verification: Sum equals total inventory items: True\n"
     ]
    }
   ],
   "source": [
    "print(\"2. ORDER_ITEMS TABLE - Analyzing item-level fulfillment data\")\n",
    "\n",
    "# Analyze missing data in order_items by status\n",
    "order_items_analysis = order_items.groupby('status').agg({\n",
    "    'shipped_at': lambda x: x.isnull().sum(),\n",
    "    'delivered_at': lambda x: x.isnull().sum(),\n",
    "    'returned_at': lambda x: x.isnull().sum()\n",
    "}).astype(int)\n",
    "\n",
    "order_items_analysis.columns = ['Missing_Shipped', 'Missing_Delivered', 'Missing_Returned']\n",
    "order_items_analysis['Total_Items'] = order_items.groupby('status').size()\n",
    "\n",
    "print(\"\\nMissing fulfillment dates in order_items by status:\")\n",
    "print(order_items_analysis)\n",
    "\n",
    "# Check if order_items missing data aligns with parent orders\n",
    "print(\"\\n\\n3. INVENTORY_ITEMS TABLE - Analyzing sold_at missing data\")\n",
    "\n",
    "# For inventory_items, missing sold_at likely means items haven't been sold yet\n",
    "inventory_sold_analysis = pd.DataFrame({\n",
    "    'Total_Inventory_Items': [len(inventory_items)],\n",
    "    'Items_with_sold_at': [inventory_items['sold_at'].notna().sum()],\n",
    "    'Items_NOT_sold': [inventory_items['sold_at'].isnull().sum()],\n",
    "    'Percentage_Unsold': [(inventory_items['sold_at'].isnull().sum() / len(inventory_items) * 100).round(1)]\n",
    "})\n",
    "\n",
    "print(\"\\nInventory items sold status:\")\n",
    "print(inventory_sold_analysis)\n",
    "\n",
    "# Calculate and print the sum of Items_with_sold_at and Items_NOT_sold\n",
    "items_sold = inventory_sold_analysis['Items_with_sold_at'].iloc[0]\n",
    "items_not_sold = inventory_sold_analysis['Items_NOT_sold'].iloc[0]\n",
    "total_sum = items_sold + items_not_sold\n",
    "\n",
    "print(f\"\\nItems with sold_at: {items_sold:,}\")\n",
    "print(f\"Items NOT sold: {items_not_sold:,}\")\n",
    "print(f\"Sum of both: {total_sum:,}\")\n",
    "print(f\"Total inventory items (verification): {len(inventory_items):,}\")\n",
    "print(f\"\\nVerification: Sum equals total inventory items: {total_sum == len(inventory_items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0480df3",
   "metadata": {},
   "source": [
    "**Order Items Processing Flow**\n",
    "- **Cancelled orders**: 100% missing all fulfillment dates (27,190 items)\n",
    "- **Processing orders**: 100% missing all dates as they haven't been shipped (35,839 items)\n",
    "- **Shipped orders**: Have shipping dates but await delivery (54,631 items)\n",
    "- **Complete orders**: Full shipping and delivery information (45,955 items) \n",
    "- **Returned items**: Complete history across all date fields (17,963 items)\n",
    "\n",
    "**Inventory Management Status**\n",
    "- **37% of inventory sold**: 181,578 items have sold_at dates\n",
    "- **63% of inventory unsold**: 308,598 items remain available in stock\n",
    "- **Total inventory**: 490,176 items\n",
    "\n",
    "These patterns indicate that missing date values represent meaningful business states in the order processing and inventory management workflows rather than data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0255199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. PRODUCTS TABLE - Analyzing missing product data\n",
      "\n",
      "Missing data in products table:\n",
      "       Missing_Count  Missing_Percentage\n",
      "brand             24                0.08\n",
      "name               2                0.01\n",
      "\n",
      "Analyzing patterns in missing product data:\n",
      "\n",
      "brand missing by category (top 5):\n",
      "category\n",
      "Intimates            4\n",
      "Tops & Tees          4\n",
      "Outerwear & Coats    3\n",
      "Swim                 2\n",
      "Accessories          2\n",
      "Name: brand, dtype: int64\n",
      "\n",
      "brand missing by department:\n",
      "department\n",
      "Men      12\n",
      "Women    12\n",
      "Name: brand, dtype: int64\n",
      "\n",
      "name missing by category (top 5):\n",
      "category\n",
      "Intimates            1\n",
      "Outerwear & Coats    1\n",
      "Name: name, dtype: int64\n",
      "\n",
      "name missing by department:\n",
      "department\n",
      "Men      1\n",
      "Women    1\n",
      "Name: name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"4. PRODUCTS TABLE - Analyzing missing product data\")\n",
    "\n",
    "# Analyze missing data patterns in products\n",
    "products_missing = products.isnull().sum()\n",
    "products_missing_pct = (products_missing / len(products) * 100).round(2)\n",
    "\n",
    "products_missing_df = pd.DataFrame({\n",
    "    'Missing_Count': products_missing,\n",
    "    'Missing_Percentage': products_missing_pct\n",
    "})\n",
    "products_missing_df = products_missing_df[products_missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"\\nMissing data in products table:\")\n",
    "print(products_missing_df)\n",
    "\n",
    "# Check if missing data correlates with certain categories or departments\n",
    "if not products_missing_df.empty:\n",
    "    print(\"\\nAnalyzing patterns in missing product data:\")\n",
    "    \n",
    "    # Check missing data by category\n",
    "    for col in products_missing_df.index:\n",
    "        if col in products.columns:\n",
    "            missing_by_category = products.groupby('category')[col].apply(lambda x: x.isnull().sum())\n",
    "            missing_by_category = missing_by_category[missing_by_category > 0].sort_values(ascending=False)\n",
    "            \n",
    "            if not missing_by_category.empty:\n",
    "                print(f\"\\n{col} missing by category (top 5):\")\n",
    "                print(missing_by_category.head())\n",
    "            \n",
    "            # Check missing data by department\n",
    "            missing_by_dept = products.groupby('department')[col].apply(lambda x: x.isnull().sum())\n",
    "            missing_by_dept = missing_by_dept[missing_by_dept > 0].sort_values(ascending=False)\n",
    "            \n",
    "            if not missing_by_dept.empty:\n",
    "                print(f\"\\n{col} missing by department:\")\n",
    "                print(missing_by_dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8484b",
   "metadata": {},
   "source": [
    "The products table shows excellent data quality with very limited missing information. The few missing values are evenly distributed across departments, suggesting random omissions rather than systematic data issues. table shows excellent data quality with very limited missing information. The few missing values are evenly distributed across departments, suggesting random omissions rather than systematic data issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5df228",
   "metadata": {},
   "source": [
    "### Addressing Missing `user_id` in Events Table\n",
    "\n",
    "Nearly half of the `events` records have a missing `user_id`. This is expected in web analytics data, as it can represents anonymous visitors who have not registered or logged in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7112ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique user_ids in events (non-null): 79,989\n",
      "user_ids in users table: 100,000\n",
      "user_ids in events not found in users table: 0\n",
      "Unique user_ids in orders: 79,989\n"
     ]
    }
   ],
   "source": [
    "# Check if all unique user_ids in events (excluding missing) exist in users table\n",
    "event_user_ids = set(events['user_id'].dropna().unique())\n",
    "user_table_ids = set(users['id'].unique())\n",
    "missing_in_users = event_user_ids - user_table_ids\n",
    "\n",
    "print(f\"Unique user_ids in events (non-null): {len(event_user_ids):,}\")\n",
    "print(f\"user_ids in users table: {len(user_table_ids):,}\")\n",
    "print(f\"user_ids in events not found in users table: {len(missing_in_users):,}\")\n",
    "\n",
    "# Compare unique user_ids in events to unique purchasing users\n",
    "purchasing_user_ids = set(orders['user_id'].unique())\n",
    "print(f\"Unique user_ids in orders: {len(purchasing_user_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eabeb8",
   "metadata": {},
   "source": [
    "- All non-null `user_id` values in the `events` table are present in the `users` table (0 missing).\n",
    "- The number of unique `user_id` values in `events` (79,989) matches the number of unique `user_id` values in `orders` (79,989), but is less than the total number of users (100,000).\n",
    "- **Conclusion:** \n",
    "  - All events with a `user_id` are correctly linked to registered users.\n",
    "  - The difference between total users and users with events/orders means some users have never logged in or performed any tracked event/order.\n",
    "  - **Missing `user_id` in events is not a data error, but represents anonymous (unregistered or not-logged-in) visitors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287f565",
   "metadata": {},
   "source": [
    "## Business and Performance Metrics Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b88fa6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KEY BUSINESS METRICS ===\n",
      "Total Users: 100,000\n",
      "Total Orders: 125,278\n",
      "Total Products: 29,120\n",
      "Total Revenue: $10,788,195.71\n",
      "\n",
      "=== PERFORMANCE METRICS ===\n",
      "Users with Orders: 79,989\n",
      "Conversion Rate: 79.99%\n",
      "Average Order Value: $86.11\n",
      "Median Order Value: $55.03\n",
      "Revenue per User: $107.88\n",
      "Revenue per Customer: $134.87\n",
      "\n",
      "=== ORDER PATTERNS ===\n",
      "Avg Orders per Customer: 1.57\n",
      "Max Orders by Single Customer: 4\n",
      "% of One-time Customers: 62.25%\n",
      "Repeat Customer Rate: 37.75%\n",
      "Order Return Rate: 9.93%\n",
      "Average Items per Order: 1.45\n",
      "Churn Rate (no order in last 6 months): 65.38%\n"
     ]
    }
   ],
   "source": [
    "print(\"=== KEY BUSINESS METRICS ===\")\n",
    "\n",
    "# Basic counts\n",
    "total_users = users['id'].nunique()\n",
    "total_orders = orders['order_id'].nunique()\n",
    "total_products = products['id'].nunique()\n",
    "total_revenue = order_items['sale_price'].sum()\n",
    "\n",
    "print(f\"Total Users: {total_users:,}\")\n",
    "print(f\"Total Orders: {total_orders:,}\")\n",
    "print(f\"Total Products: {total_products:,}\")\n",
    "print(f\"Total Revenue: ${total_revenue:,.2f}\")\n",
    "\n",
    "# Calculate key metrics\n",
    "users_with_orders = orders['user_id'].nunique()\n",
    "conversion_rate = (users_with_orders / total_users) * 100\n",
    "aov = total_revenue / total_orders\n",
    "median_order_value = order_items.groupby('order_id')['sale_price'].sum().median()\n",
    "revenue_per_user = total_revenue / total_users\n",
    "revenue_per_customer = total_revenue / users_with_orders\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE METRICS ===\")\n",
    "print(f\"Users with Orders: {users_with_orders:,}\")\n",
    "print(f\"Conversion Rate: {conversion_rate:.2f}%\")\n",
    "print(f\"Average Order Value: ${aov:.2f}\")\n",
    "print(f\"Median Order Value: ${median_order_value:.2f}\")\n",
    "print(f\"Revenue per User: ${revenue_per_user:.2f}\")\n",
    "print(f\"Revenue per Customer: ${revenue_per_customer:.2f}\")\n",
    "\n",
    "# Order statistics\n",
    "orders_per_customer = orders.groupby('user_id').size()\n",
    "repeat_customers = (orders_per_customer > 1).sum()\n",
    "repeat_customer_rate = (repeat_customers / users_with_orders) * 100\n",
    "returned_orders = orders[orders['status'].str.lower().str.contains('return')]['order_id'].nunique()\n",
    "order_return_rate = (returned_orders / total_orders) * 100 if total_orders else 0\n",
    "items_per_order = order_items.groupby('order_id').size().mean()\n",
    "cutoff_date = (pd.Timestamp(datetime.date.today()) - pd.DateOffset(months=6)).tz_localize('UTC')\n",
    "last_order_dates = orders.groupby('user_id')['created_at'].max()\n",
    "churned_users = (last_order_dates < cutoff_date).sum()\n",
    "churn_rate = (churned_users / users_with_orders) * 100 if users_with_orders else 0\n",
    "print(f\"\\n=== ORDER PATTERNS ===\")\n",
    "print(f\"Avg Orders per Customer: {orders_per_customer.mean():.2f}\")\n",
    "print(f\"Max Orders by Single Customer: {orders_per_customer.max()}\")\n",
    "print(f\"% of One-time Customers: {(orders_per_customer == 1).mean() * 100:.2f}%\")\n",
    "print(f\"Repeat Customer Rate: {repeat_customer_rate:.2f}%\")\n",
    "print(f\"Order Return Rate: {order_return_rate:.2f}%\")\n",
    "print(f\"Average Items per Order: {items_per_order:.2f}\")\n",
    "print(f\"Churn Rate (no order in last 6 months): {churn_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e52e095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time to Ship: 1.50 days\n",
      "Average Time to Deliver: 2.49 days\n",
      "\n",
      "Customer Segmentation:\n",
      "  One-time Customers: 49,795 (49.80%)\n",
      "  Repeat Customers: 30,194 (30.19%)\n",
      "\n",
      "Conversion Rate by Traffic Source:\n",
      "  Display: 79.69% (3189/4002)\n",
      "  Email: 80.37% (3917/4874)\n",
      "  Facebook: 79.90% (4930/6170)\n",
      "  Organic: 80.16% (11883/14824)\n",
      "  Search: 79.95% (56070/70130)\n",
      "\n",
      "Order Status Distribution:\n",
      "  Shipped: 37,660 orders (30.1%)\n",
      "  Complete: 31,614 orders (25.2%)\n",
      "  Processing: 24,845 orders (19.8%)\n",
      "  Cancelled: 18,714 orders (14.9%)\n",
      "  Returned: 12,445 orders (9.9%)\n",
      "\n",
      "Average days to sell: 29.6\n",
      "Median days to sell: 30.0\n",
      "Fastest sale: 0.0 days, Slowest sale: 59.0 days\n",
      "\n",
      "Sold Inventory: 181,578\n",
      "Unsold Inventory: 308,598\n",
      "Turnover Rate: 37.04%\n"
     ]
    }
   ],
   "source": [
    "# Other performance metrics\n",
    "# Average Shipping and Delivery Time (in days)\n",
    "orders_shipped = orders[orders['shipped_at'].notna() & orders['created_at'].notna()]\n",
    "orders_delivered = orders[orders['delivered_at'].notna() & orders['shipped_at'].notna()]\n",
    "avg_ship_time = (orders_shipped['shipped_at'] - orders_shipped['created_at']).dt.total_seconds().mean() / 86400\n",
    "print(f\"Average Time to Ship: {avg_ship_time:.2f} days\")\n",
    "avg_delivery_time = (orders_delivered['delivered_at'] - orders_delivered['shipped_at']).dt.total_seconds().mean() / 86400\n",
    "print(f\"Average Time to Deliver: {avg_delivery_time:.2f} days\")\n",
    "    \n",
    "# Customer Segmentation: Repeat vs. One-time Customers\n",
    "one_time = (orders_per_customer == 1).sum()\n",
    "repeat = (orders_per_customer > 1).sum()\n",
    "print(f\"\\nCustomer Segmentation:\")\n",
    "print(f\"  One-time Customers: {one_time:,} ({(one_time/total_users)*100:.2f}%)\")\n",
    "print(f\"  Repeat Customers: {repeat:,} ({(repeat/total_users)*100:.2f}%)\")\n",
    "    \n",
    "# Conversion Rate by Traffic Source\n",
    "traffic_conv = users.merge(orders[['user_id']], left_on='id', right_on='user_id', how='left', indicator=True)\n",
    "traffic_conv['is_buyer'] = traffic_conv['_merge'] == 'both'\n",
    "traffic_conv_deduped = traffic_conv.drop_duplicates('id')\n",
    "conv_by_source = traffic_conv_deduped.groupby('traffic_source').agg(\n",
    "    total_users=('id','count'),\n",
    "    buyers=('is_buyer','sum')\n",
    ")\n",
    "conv_by_source['conversion_rate'] = (conv_by_source['buyers'] / conv_by_source['total_users']) * 100\n",
    "print(\"\\nConversion Rate by Traffic Source:\")\n",
    "for idx, row in conv_by_source.iterrows():\n",
    "    print(f\"  {idx}: {row['conversion_rate']:.2f}% ({int(row['buyers'])}/{int(row['total_users'])})\")\n",
    "    \n",
    "# Order status distribution\n",
    "status_counts = orders['status'].value_counts()\n",
    "print(\"\\nOrder Status Distribution:\")\n",
    "for status, count in status_counts.items():\n",
    "    pct = (count / total_orders) * 100\n",
    "    print(f\"  {status}: {count:,} orders ({pct:.1f}%)\")\n",
    "    \n",
    "\n",
    "# Inventory Turnover Analysis\n",
    "inventory_items['created_at'] = pd.to_datetime(inventory_items['created_at'], errors='coerce')\n",
    "inventory_items['sold_at'] = pd.to_datetime(inventory_items['sold_at'], errors='coerce')\n",
    "inventory_items['days_to_sell'] = (inventory_items['sold_at'] - inventory_items['created_at']).dt.days\n",
    "sold_items = inventory_items[inventory_items['days_to_sell'].notna()]\n",
    "print(f\"\\nAverage days to sell: {sold_items['days_to_sell'].mean():.1f}\")\n",
    "print(f\"Median days to sell: {sold_items['days_to_sell'].median():.1f}\")\n",
    "print(f\"Fastest sale: {sold_items['days_to_sell'].min()} days, Slowest sale: {sold_items['days_to_sell'].max()} days\")\n",
    "\n",
    "# Inventory turnover and unsold inventory\n",
    "sold_items = inventory_items['sold_at'].notna().sum()\n",
    "unsold_items = inventory_items['sold_at'].isna().sum()\n",
    "total_inventory = len(inventory_items)\n",
    "turnover_rate = sold_items / total_inventory * 100 if total_inventory else 0\n",
    "print(f\"\\nSold Inventory: {sold_items:,}\")\n",
    "print(f\"Unsold Inventory: {unsold_items:,}\")\n",
    "print(f\"Turnover Rate: {turnover_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10ea78",
   "metadata": {},
   "source": [
    "## Top Customers, Categories, Brands, and Products by Revenue and Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51b4496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Customers by Lifetime Value (CLV):\n",
      "  Charles Spencer: $2,015.92\n",
      "  Devin White: $1,626.13\n",
      "  Robert Gonzalez: $1,535.02\n",
      "  Joseph Stewart: $1,520.93\n",
      "  Jennifer Ramos: $1,490.99\n",
      "\n",
      "Top 5 Product Categories by Revenue:\n",
      "  Outerwear & Coats: $1,291,409.80\n",
      "  Jeans: $1,255,639.76\n",
      "  Sweaters: $830,785.40\n",
      "  Swim: $647,627.43\n",
      "  Suits & Sport Coats: $647,153.19\n",
      "\n",
      "Top 5 Product Categories by Order Count:\n",
      "  Intimates: 12,713 orders\n",
      "  Jeans: 12,434 orders\n",
      "  Tops & Tees: 11,564 orders\n",
      "  Fashion Hoodies & Sweatshirts: 11,344 orders\n",
      "  Swim: 11,076 orders\n",
      "\n",
      "Top 5 Brands by Revenue:\n",
      "  Diesel: $201,557.41\n",
      "  Calvin Klein: $201,503.74\n",
      "  True Religion: $179,670.11\n",
      "  7 For All Mankind: $172,409.43\n",
      "  Carhartt: $169,504.77\n",
      "\n",
      "Top 5 Brands by Order Count:\n",
      "  Allegra K: 6,130 orders\n",
      "  Calvin Klein: 3,112 orders\n",
      "  Carhartt: 2,506 orders\n",
      "  Hanes: 1,956 orders\n",
      "  Volcom: 1,850 orders\n",
      "\n",
      "Top 5 Products by Revenue:\n",
      "  The North Face Apex Bionic Soft Shell Jacket - Men's (Price: $903.00): $11,739.00\n",
      "  ASCIS Cushion Low Socks (Pack of 3) (Price: $903.00): $9,933.00\n",
      "  The North Face Apex Bionic Jacket - Men's (Price: $903.00): $8,127.00\n",
      "  The North Face Apex Bionic Soft Shell Jacket - Men's (Price: $903.00): $7,224.00\n",
      "  JORDAN DURASHEEN SHORT MENS 404309-109 (Price: $903.00): $7,224.00\n",
      "\n",
      "Top 5 Products by Order Amount:\n",
      "  G-Star Men's Cm Rovic Arc 3D Loose Tapered Pant (Price: $95.00): 20 orders\n",
      "  Woolrich Men's Elite Waterproof Breathable Tactical Parka Jacket (Price: $180.85): 19 orders\n",
      "  Volcom Men's Maguro Block Boardshort (Price: $49.50): 18 orders\n",
      "  Oakley Originate Fleece Hoodie 2013 (Price: $99.95): 18 orders\n",
      "  Shapewear Smoothing Slimming Control Bodysuit (Price: $39.95): 17 orders\n"
     ]
    }
   ],
   "source": [
    "# Top 5 Customer Lifetime Value (CLV)\n",
    "customer_revenue = orders.merge(order_items, left_on='order_id', right_on='order_id')\n",
    "grouped_customers = customer_revenue.groupby('user_id_x')['sale_price'].sum().sort_values(ascending=False)\n",
    "top_customers = users.set_index('id').loc[grouped_customers.head(5).index]\n",
    "top_customers = top_customers[['first_name', 'last_name']].copy()\n",
    "top_customers['revenue'] = grouped_customers.head(5).values\n",
    "print(\"Top 5 Customers by Lifetime Value (CLV):\")\n",
    "for i, row in top_customers.iterrows():\n",
    "    print(f\"  {row['first_name']} {row['last_name']}: ${row['revenue']:,.2f}\")\n",
    "\n",
    "# Top 5 category by revenue\n",
    "category_revenue = order_items.merge(products[['id','category']], left_on='product_id', right_on='id')\n",
    "category_revenue = category_revenue.groupby('category')['sale_price'].sum().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 Product Categories by Revenue:\")\n",
    "for cat, val in category_revenue.head(5).items():\n",
    "    print(f\"  {cat}: ${val:,.2f}\")\n",
    "\n",
    "# Top 5 category by order count (popularity)\n",
    "category_popularity = order_items.merge(products[['id','category']], left_on='product_id', right_on='id')\n",
    "category_order_count = category_popularity.groupby('category')['order_id'].nunique().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 Product Categories by Order Count:\")\n",
    "for cat, count in category_order_count.head(5).items():\n",
    "    print(f\"  {cat}: {count:,} orders\")\n",
    "\n",
    "# Top 5 brand by revenue\n",
    "brand_revenue = order_items.merge(products[['id','brand']], left_on='product_id', right_on='id')\n",
    "brand_revenue = brand_revenue.groupby('brand')['sale_price'].sum().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 Brands by Revenue:\")\n",
    "for brand, val in brand_revenue.head(5).items():\n",
    "    print(f\"  {brand}: ${val:,.2f}\")\n",
    "\n",
    "# Top 5 brand by order count (popularity)\n",
    "brand_popularity = order_items.merge(products[['id','brand']], left_on='product_id', right_on='id')\n",
    "brand_order_count = brand_popularity.groupby('brand')['order_id'].nunique().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 Brands by Order Count:\")\n",
    "for brand, count in brand_order_count.head(5).items():\n",
    "    print(f\"  {brand}: {count:,} orders\")\n",
    "\n",
    "# Top 5 products by revenue\n",
    "product_revenue = order_items.groupby('product_id')['sale_price'].sum().sort_values(ascending=False)\n",
    "top_products = products.set_index('id').loc[product_revenue.head(5).index]\n",
    "top_products = top_products[['name', 'retail_price']].copy()\n",
    "top_products['revenue'] = product_revenue.head(5).values\n",
    "print(\"\\nTop 5 Products by Revenue:\")\n",
    "for i, row in top_products.iterrows():\n",
    "    print(f\"  {row['name']} (Price: ${row['retail_price']:.2f}): ${row['revenue']:,.2f}\")\n",
    "\n",
    "# Top 5 products by order count (popularity)\n",
    "product_order_count = order_items['product_id'].value_counts().head(5)\n",
    "top_products_count = products.set_index('id').loc[product_order_count.index]\n",
    "top_products_count = top_products_count[['name', 'retail_price']].copy()\n",
    "top_products_count['order_count'] = product_order_count.values\n",
    "print(\"\\nTop 5 Products by Order Amount:\")\n",
    "for i, row in top_products_count.iterrows():\n",
    "    print(f\"  {row['name']} (Price: ${row['retail_price']:.2f}): {row['order_count']:,} orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390665d5",
   "metadata": {},
   "source": [
    "### Profitability and Bulk vs. Retail Order Analysis\n",
    "This section explores profit margins by brand and category, and compares the profitability and frequency of bulk versus retail orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea9ba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Gross Margin by Brand (top 10):\n",
      "brand\n",
      "CTR Specialties        0.664\n",
      "Voom                   0.663\n",
      "Iisli                  0.661\n",
      "Material Girl          0.659\n",
      "Aris A                 0.653\n",
      "NygÃ¥rd Collection     0.652\n",
      "RAY&LI                 0.651\n",
      "HodoHome Loungewear    0.649\n",
      "Sheer Delights         0.649\n",
      "Brighton               0.648\n",
      "Name: gross_margin, dtype: float64\n",
      "\n",
      "Average Gross Margin by Product Category (top 10):\n",
      "category\n",
      "Blazers & Jackets      0.619954\n",
      "Skirts                 0.600223\n",
      "Suits & Sport Coats    0.599325\n",
      "Accessories            0.599295\n",
      "Socks & Hosiery        0.598727\n",
      "Active                 0.579527\n",
      "Maternity              0.558457\n",
      "Outerwear & Coats      0.554780\n",
      "Dresses                0.548724\n",
      "Pants                  0.541036\n",
      "Name: gross_margin, dtype: float64\n",
      "\n",
      "Total Profit and Average Gross Margin by Category (top 10):\n",
      "                                total_profit  avg_gross_margin\n",
      "category                                                      \n",
      "Outerwear & Coats              717161.830527          0.554780\n",
      "Jeans                          583438.360933          0.464160\n",
      "Sweaters                       430714.163932          0.521427\n",
      "Suits & Sport Coats            388143.968386          0.599325\n",
      "Swim                           318929.527349          0.488606\n",
      "Fashion Hoodies & Sweatshirts  307408.862179          0.487988\n",
      "Sleep & Lounge                 287375.814640          0.506166\n",
      "Active                         269215.579782          0.579527\n",
      "Shorts                         257843.944730          0.499415\n",
      "Accessories                    251049.169209          0.599295\n",
      "\n",
      "Total Profit and Average Gross Margin by Brand (top 10):\n",
      "                    total_profit  avg_gross_margin\n",
      "brand                                             \n",
      "Calvin Klein       107162.834540          0.515103\n",
      "Diesel              99966.335008          0.497005\n",
      "Carhartt            90217.664163          0.522866\n",
      "True Religion       85939.947966          0.481752\n",
      "7 For All Mankind   82364.490380          0.475761\n",
      "Tommy Hilfiger      63859.588574          0.518602\n",
      "Ray-Ban             57127.590363          0.576615\n",
      "Columbia            55985.534861          0.540178\n",
      "The North Face      55892.590348          0.554361\n",
      "Volcom              53526.533880          0.485240\n",
      "\n",
      "Correlation between total profit and average gross margin (category): 0.284\n",
      "Correlation between total profit and average gross margin (brand): 0.013\n",
      "\n",
      "Order Type Frequency:\n",
      "order_type\n",
      "Retail    87805\n",
      "Bulk      37473\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bulk vs. Retail Order Profitability:\n",
      "            avg_order_margin  avg_item_margin  order_count\n",
      "order_type                                                \n",
      "Bulk                0.513186         0.510720        37473\n",
      "Retail              0.510341         0.510341        87805\n"
     ]
    }
   ],
   "source": [
    "# Calculate gross margin if not already present\n",
    "if 'gross_margin' not in products.columns:\n",
    "    products['gross_margin'] = (products['retail_price'] - products['cost']) / products['retail_price']\n",
    "\n",
    "# Gross margin by brand and product category (mean)\n",
    "brand_margin = products.groupby('brand')['gross_margin'].mean().sort_values(ascending=False)\n",
    "category_margin = products.groupby('category')['gross_margin'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nAverage Gross Margin by Brand (top 10):\")\n",
    "print(brand_margin.head(10))\n",
    "print(\"\\nAverage Gross Margin by Product Category (top 10):\")\n",
    "print(category_margin.head(10))\n",
    "\n",
    "# --- EDA on Profit Margin and Total Profit by Category/Brand ---\n",
    "# Merge order_items with products to get category, brand, and cost\n",
    "order_items_profit = order_items.merge(products[['id', 'category', 'brand', 'cost']], left_on='product_id', right_on='id')\n",
    "\n",
    "# Calculate profit per item\n",
    "order_items_profit['profit'] = order_items_profit['sale_price'] - order_items_profit['cost']\n",
    "\n",
    "# Total profit by category\n",
    "profit_by_category = order_items_profit.groupby('category')['profit'].sum().sort_values(ascending=False)\n",
    "# Average margin by category (already calculated as category_margin)\n",
    "category_profit_margin = pd.DataFrame({\n",
    "    'total_profit': profit_by_category,\n",
    "    'avg_gross_margin': category_margin\n",
    "}).sort_values('total_profit', ascending=False)\n",
    "\n",
    "print(\"\\nTotal Profit and Average Gross Margin by Category (top 10):\")\n",
    "print(category_profit_margin.head(10))\n",
    "\n",
    "# Total profit by brand\n",
    "profit_by_brand = order_items_profit.groupby('brand')['profit'].sum().sort_values(ascending=False)\n",
    "# Average margin by brand (already calculated as brand_margin)\n",
    "brand_profit_margin = pd.DataFrame({\n",
    "    'total_profit': profit_by_brand,\n",
    "    'avg_gross_margin': brand_margin\n",
    "}).sort_values('total_profit', ascending=False)\n",
    "\n",
    "print(\"\\nTotal Profit and Average Gross Margin by Brand (top 10):\")\n",
    "print(brand_profit_margin.head(10))\n",
    "\n",
    "# Correlation between average gross margin and total profit\n",
    "cat_corr = category_profit_margin['total_profit'].corr(category_profit_margin['avg_gross_margin'])\n",
    "brand_corr = brand_profit_margin['total_profit'].corr(brand_profit_margin['avg_gross_margin'])\n",
    "\n",
    "print(f\"\\nCorrelation between total profit and average gross margin (category): {cat_corr:.3f}\")\n",
    "print(f\"Correlation between total profit and average gross margin (brand): {brand_corr:.3f}\")\n",
    "\n",
    "# --- Bulk vs. Retail Order Analysis ---\n",
    "# Define bulk order as num_of_item > 1, retail as num_of_item == 1\n",
    "orders['order_type'] = orders['num_of_item'].apply(lambda x: 'Bulk' if x > 1 else 'Retail')\n",
    "\n",
    "# Frequency of bulk vs retail orders\n",
    "order_type_counts = orders['order_type'].value_counts()\n",
    "print(\"\\nOrder Type Frequency:\")\n",
    "print(order_type_counts)\n",
    "\n",
    "# Profit margin per order: sum sale_price / sum retail_price for items in each order\n",
    "order_items_merged = order_items.merge(products[['id', 'retail_price', 'cost']], left_on='product_id', right_on='id')\n",
    "order_items_merged['item_margin'] = (order_items_merged['sale_price'] - order_items_merged['cost']) / order_items_merged['sale_price']\n",
    "\n",
    "# Aggregate margin by order\n",
    "order_margin = order_items_merged.groupby('order_id').agg(\n",
    "    total_sale_price=('sale_price', 'sum'),\n",
    "    total_cost=('cost', 'sum'),\n",
    "    avg_item_margin=('item_margin', 'mean')\n",
    ")\n",
    "order_margin['order_margin'] = (order_margin['total_sale_price'] - order_margin['total_cost']) / order_margin['total_sale_price']\n",
    "\n",
    "# Merge order_type into order_margin\n",
    "order_margin = order_margin.merge(orders[['order_id', 'order_type']], left_index=True, right_on='order_id')\n",
    "\n",
    "# Compare average margin and frequency\n",
    "bulk_stats = order_margin.groupby('order_type').agg(\n",
    "    avg_order_margin=('order_margin', 'mean'),\n",
    "    avg_item_margin=('avg_item_margin', 'mean'),\n",
    "    order_count=('order_id', 'count')\n",
    ")\n",
    "print(\"\\nBulk vs. Retail Order Profitability:\")\n",
    "print(bulk_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f49503",
   "metadata": {},
   "source": [
    "## Customer Demography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bd78fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CUSTOMER DEMOGRAPHY ===\n",
      "\n",
      "Gender Distribution:\n",
      "  F: 50,172 (50.17%)\n",
      "  M: 49,828 (49.83%)\n",
      "\n",
      "Age Statistics:\n",
      "  Mean Age: 41.0\n",
      "  Median Age: 41.0\n",
      "  Min Age: 12\n",
      "  Max Age: 70\n",
      "  Age Groups:\n",
      "    <18: 10,195 (10.20%)\n",
      "    18-24: 11,756 (11.76%)\n",
      "    25-34: 16,976 (16.98%)\n",
      "    35-44: 16,828 (16.83%)\n",
      "    45-54: 16,980 (16.98%)\n",
      "    55-64: 17,152 (17.15%)\n",
      "    65+: 10,113 (10.11%)\n",
      "\n",
      "Top 5 Countrys by User Count:\n",
      "  China: 33,821\n",
      "  United States: 22,516\n",
      "  Brasil: 14,511\n",
      "  South Korea: 5,304\n",
      "  United Kingdom: 4,711\n",
      "\n",
      "Top 5 States by User Count:\n",
      "  Guangdong: 5,425\n",
      "  England: 4,170\n",
      "  California: 3,745\n",
      "  Texas: 2,513\n",
      "  Shanghai: 2,423\n",
      "\n",
      "Top 5 Citys by User Count:\n",
      "  Shanghai: 2,483\n",
      "  Beijing: 2,042\n",
      "  Seoul: 1,444\n",
      "  Shenzhen: 1,292\n",
      "  null: 992\n",
      "\n",
      "Traffic Source Distribution:\n",
      "  Search: 70,130 (70.13%)\n",
      "  Organic: 14,824 (14.82%)\n",
      "  Facebook: 6,170 (6.17%)\n",
      "  Email: 4,874 (4.87%)\n",
      "  Display: 4,002 (4.00%)\n",
      "\n",
      "Average Orders per User by Gender and Age Group:\n",
      "age_group   <18  18-24  25-34  35-44  45-54  55-64   65+\n",
      "gender                                                  \n",
      "F          1.23   1.24   1.26   1.25   1.24   1.28  1.25\n",
      "M          1.25   1.26   1.26   1.26   1.24   1.24  1.26\n"
     ]
    }
   ],
   "source": [
    "# --- Customer Demography EDA ---\n",
    "print(\"=== CUSTOMER DEMOGRAPHY ===\")\n",
    "\n",
    "# Gender distribution\n",
    "gender_counts = users['gender'].value_counts(dropna=False)\n",
    "gender_pct = users['gender'].value_counts(normalize=True, dropna=False) * 100\n",
    "print(\"\\nGender Distribution:\")\n",
    "for g, c in gender_counts.items():\n",
    "    print(f\"  {g}: {c:,} ({gender_pct[g]:.2f}%)\")\n",
    "\n",
    "# Age distribution\n",
    "print(\"\\nAge Statistics:\")\n",
    "print(f\"  Mean Age: {users['age'].mean():.1f}\")\n",
    "print(f\"  Median Age: {users['age'].median():.1f}\")\n",
    "print(f\"  Min Age: {users['age'].min()}\")\n",
    "print(f\"  Max Age: {users['age'].max()}\")\n",
    "print(\"  Age Groups:\")\n",
    "age_bins = [0, 18, 25, 35, 45, 55, 65, 100]\n",
    "age_labels = ['<18', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "users['age_group'] = pd.cut(users['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "age_group_counts = users['age_group'].value_counts(sort=False)\n",
    "for group, count in age_group_counts.items():\n",
    "    pct = (count / len(users)) * 100\n",
    "    print(f\"    {group}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Geographic distribution (country, state, city)\n",
    "for col in ['country', 'state', 'city']:\n",
    "    if col in users.columns:\n",
    "        top_geo = users[col].value_counts().head(5)\n",
    "        print(f\"\\nTop 5 {col.title()}s by User Count:\")\n",
    "        for val, cnt in top_geo.items():\n",
    "            print(f\"  {val}: {cnt:,}\")\n",
    "    else:\n",
    "        print(f\"\\nNo {col} data available.\")\n",
    "\n",
    "# Traffic source breakdown\n",
    "traffic_counts = users['traffic_source'].value_counts()\n",
    "traffic_pct = users['traffic_source'].value_counts(normalize=True) * 100\n",
    "print(\"\\nTraffic Source Distribution:\")\n",
    "for src, cnt in traffic_counts.items():\n",
    "    print(f\"  {src}: {cnt:,} ({traffic_pct[src]:.2f}%)\")\n",
    "\n",
    "# Customer segmentation by demography and purchasing\n",
    "user_orders = orders.groupby('user_id').size().rename('order_count')\n",
    "users_demo = users.join(user_orders, how='left').fillna({'order_count': 0})\n",
    "print(\"\\nAverage Orders per User by Gender and Age Group:\")\n",
    "seg = users_demo.groupby(['gender', 'age_group'], observed=True)['order_count'].mean().unstack()\n",
    "print(seg.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7662bf7",
   "metadata": {},
   "source": [
    "## Advanced EDA: Time Series, Basket, Funnel, Product, and Customer Analysis\n",
    "This section covers advanced exploratory data analysis, including time series trends, product bundling, funnel analysis, customer lifetime value segmentation, geographic revenue, and returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a776ac",
   "metadata": {},
   "source": [
    "### Time Series Analysis: Monthly Trends for Users, Orders, Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e24b4b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== MONTHLY TRENDS (LAST 12 MONTHS) =====\n",
      "\n",
      "Monthly New Users:\n",
      "created_month\n",
      "2024-06    1219\n",
      "2024-07    1303\n",
      "2024-08    1294\n",
      "2024-09    1296\n",
      "2024-10    1357\n",
      "2024-11    1247\n",
      "2024-12    1286\n",
      "2025-01    1256\n",
      "2025-02    1193\n",
      "2025-03    1238\n",
      "2025-04    1252\n",
      "2025-05    3664\n",
      "Freq: M, dtype: int64\n",
      "\n",
      "Month-over-Month User Growth:\n",
      "created_month\n",
      "2024-06     -4.47\n",
      "2024-07      6.89\n",
      "2024-08     -0.69\n",
      "2024-09      0.15\n",
      "2024-10      4.71\n",
      "2024-11     -8.11\n",
      "2024-12      3.13\n",
      "2025-01     -2.33\n",
      "2025-02     -5.02\n",
      "2025-03      3.77\n",
      "2025-04      1.13\n",
      "2025-05    192.65\n",
      "Freq: M, dtype: float64 %\n",
      "\n",
      "Monthly Orders:\n",
      "created_month\n",
      "2024-06     2887\n",
      "2024-07     3226\n",
      "2024-08     3414\n",
      "2024-09     3483\n",
      "2024-10     3861\n",
      "2024-11     3969\n",
      "2024-12     4368\n",
      "2025-01     4638\n",
      "2025-02     4683\n",
      "2025-03     5734\n",
      "2025-04     6522\n",
      "2025-05    10241\n",
      "Freq: M, dtype: int64\n",
      "\n",
      "Month-over-Month Order Growth:\n",
      "created_month\n",
      "2024-06    -0.79\n",
      "2024-07    11.74\n",
      "2024-08     5.83\n",
      "2024-09     2.02\n",
      "2024-10    10.85\n",
      "2024-11     2.80\n",
      "2024-12    10.05\n",
      "2025-01     6.18\n",
      "2025-02     0.97\n",
      "2025-03    22.44\n",
      "2025-04    13.74\n",
      "2025-05    57.02\n",
      "Freq: M, dtype: float64 %\n",
      "\n",
      "Monthly Revenue:\n",
      "2024-06: $255,021.62\n",
      "2024-07: $281,787.91\n",
      "2024-08: $291,384.58\n",
      "2024-09: $310,539.48\n",
      "2024-10: $330,222.47\n",
      "2024-11: $341,067.74\n",
      "2024-12: $372,179.58\n",
      "2025-01: $393,653.06\n",
      "2025-02: $401,811.73\n",
      "2025-03: $497,677.67\n",
      "2025-04: $558,052.81\n",
      "2025-05: $889,765.47\n",
      "\n",
      "Month-over-Month Revenue Growth:\n",
      "created_month\n",
      "2024-06     2.88\n",
      "2024-07    10.50\n",
      "2024-08     3.41\n",
      "2024-09     6.57\n",
      "2024-10     6.34\n",
      "2024-11     3.28\n",
      "2024-12     9.12\n",
      "2025-01     5.77\n",
      "2025-02     2.07\n",
      "2025-03    23.86\n",
      "2025-04    12.13\n",
      "2025-05    59.44\n",
      "Freq: M, Name: sale_price, dtype: float64 %\n"
     ]
    }
   ],
   "source": [
    "# Monthly new users, orders, and revenue trends\n",
    "# Remove timezone before converting to period to avoid warnings\n",
    "users['created_month'] = users['created_at'].dt.tz_localize(None).dt.to_period('M')\n",
    "orders['created_month'] = orders['created_at'].dt.tz_localize(None).dt.to_period('M')\n",
    "\n",
    "# Create order_items with month and revenue info\n",
    "order_items_monthly = order_items.merge(\n",
    "    orders[['order_id', 'created_at']], \n",
    "    on='order_id', \n",
    "    how='left'\n",
    ")\n",
    "order_items_monthly['created_month'] = order_items_monthly['created_at_y'].dt.tz_localize(None).dt.to_period('M')\n",
    "\n",
    "# Calculate monthly metrics\n",
    "monthly_users = users.groupby('created_month').size()\n",
    "monthly_orders = orders.groupby('created_month').size()\n",
    "monthly_revenue = order_items_monthly.groupby('created_month')['sale_price'].sum()\n",
    "\n",
    "# Calculate month-over-month growth rates\n",
    "monthly_users_growth = monthly_users.pct_change() * 100\n",
    "monthly_orders_growth = monthly_orders.pct_change() * 100\n",
    "monthly_revenue_growth = monthly_revenue.pct_change() * 100\n",
    "\n",
    "# Display recent trends (last 12 months)\n",
    "print('===== MONTHLY TRENDS (LAST 12 MONTHS) =====')\n",
    "print('\\nMonthly New Users:')\n",
    "print(monthly_users.tail(12))\n",
    "print('\\nMonth-over-Month User Growth:')\n",
    "print(monthly_users_growth.tail(12).round(2), '%')\n",
    "\n",
    "print('\\nMonthly Orders:')\n",
    "print(monthly_orders.tail(12))\n",
    "print('\\nMonth-over-Month Order Growth:')\n",
    "print(monthly_orders_growth.tail(12).round(2), '%')\n",
    "\n",
    "print('\\nMonthly Revenue:')\n",
    "for month, revenue in monthly_revenue.tail(12).items():\n",
    "    print(f\"{month}: ${revenue:,.2f}\")\n",
    "print('\\nMonth-over-Month Revenue Growth:')\n",
    "print(monthly_revenue_growth.tail(12).round(2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5cf0d",
   "metadata": {},
   "source": [
    "###  Product Bundle (Market Basket) Analysis: Top Product Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ebc2547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Product Pairs Bought Together:\n",
      "  Polo Ralph Lauren Men Cashmere V-Neck Sweater + Hanes Men's Ringer Boxer Brief Fashion Assortment: 2 orders\n",
      "  Michael Kors Mens 2 Button Tan Pindot Sport Coat Jacket Blazer + Columbia Men's Hail Tech Jacket: 2 orders\n",
      "  Frank Dandy Basic Solid Mid Swim Shorts Trunks 10652 + Ray-Ban RX5245 Eyeglasses: 2 orders\n",
      "  Tie Dye Mania Retro swirl tie-dye heavyweight pullover hoodie sweatshirt + Weatherproof Adult Cross Weave Open Bottom Sweatpant. 7766: 2 orders\n",
      "  DC Men's Big Sleeve Tb + Lee Men's Rocker Short: 2 orders\n",
      "  David's Bridal Bride s Rhinestone Hooded Jacket Style DB16CSHOOD + Jones New York Women's Sweater Dress: 2 orders\n",
      "  Psychedelic Print Summer Below Knees Capri Short Leggings Stretchy Black S/M + Diesel Women's Viky-N T-shirt: 2 orders\n",
      "  Chestnut Hill Microfleece Vest - CACTUS - XS + Riggs Workwear By Wrangler Men's Big & Tall Carpenter Jean: 2 orders\n",
      "  Bali Women's Flower Underwire Bra #0180 + Cloris Murphy Sexy Sky Blue Double Edge White Lace Trim Scrunch Bun Bikini Swimsuit Beachwear BN807SB One Size Sky Blue: 2 orders\n",
      "  Wigwam Dry Foot Liner Sock + Allegra K Woman Studded Detail Elastic Waist Split Skirt Skort Shorts Black S: 2 orders\n",
      "\n",
      "Top 10 Product Category Pairs Bought Together:\n",
      "  Jeans + Tops & Tees: 746 orders\n",
      "  Jeans + Sleep & Lounge: 693 orders\n",
      "  Fashion Hoodies & Sweatshirts + Jeans: 684 orders\n",
      "  Fashion Hoodies & Sweatshirts + Tops & Tees: 681 orders\n",
      "  Sweaters + Tops & Tees: 674 orders\n",
      "  Jeans + Shorts: 670 orders\n",
      "  Jeans + Sweaters: 667 orders\n",
      "  Jeans + Swim: 662 orders\n",
      "  Shorts + Tops & Tees: 654 orders\n",
      "  Shorts + Sweaters: 647 orders\n",
      "\n",
      "Top 10 Product Brand Pairs Bought Together:\n",
      "  Allegra K + Calvin Klein: 75 orders\n",
      "  Allegra K + Hanes: 63 orders\n",
      "  Allegra K + Motherhood Maternity: 59 orders\n",
      "  Allegra K + Carhartt: 56 orders\n",
      "  Allegra K + Levi's: 53 orders\n",
      "  Allegra K + Columbia: 49 orders\n",
      "  Calvin Klein + Diesel: 47 orders\n",
      "  Allegra K + Tommy Hilfiger: 43 orders\n",
      "  Calvin Klein + Carhartt: 43 orders\n",
      "  Allegra K + Speedo: 40 orders\n"
     ]
    }
   ],
   "source": [
    "# Top product pairs bought together\n",
    "order_products = order_items.groupby('order_id')['product_id'].apply(list)\n",
    "pair_counter = Counter()\n",
    "for products_list in order_products:\n",
    "    for pair in combinations(sorted(set(products_list)), 2):\n",
    "        pair_counter[pair] += 1\n",
    "print('Top 10 Product Pairs Bought Together:')\n",
    "for (prod1, prod2), count in pair_counter.most_common(10):\n",
    "    name1 = products.loc[products['id'] == prod1, 'name'].values[0]\n",
    "    name2 = products.loc[products['id'] == prod2, 'name'].values[0]\n",
    "    print(f\"  {name1} + {name2}: {count:,} orders\")\n",
    "\n",
    "# Top product category pairs bought together\n",
    "product_id_to_category = products.set_index('id')['category'].to_dict()\n",
    "order_categories = order_items.groupby('order_id')['product_id'].apply(lambda ids: [product_id_to_category.get(pid, None) for pid in ids if pid in product_id_to_category])\n",
    "category_pair_counter = Counter()\n",
    "for categories_list in order_categories:\n",
    "    unique_cats = set([cat for cat in categories_list if cat is not None])\n",
    "    for pair in combinations(sorted(unique_cats), 2):\n",
    "        category_pair_counter[pair] += 1\n",
    "print('\\nTop 10 Product Category Pairs Bought Together:')\n",
    "for (cat1, cat2), count in category_pair_counter.most_common(10):\n",
    "    print(f\"  {cat1} + {cat2}: {count:,} orders\")\n",
    "\n",
    "# Top product brand pairs bought together\n",
    "product_id_to_brand = products.set_index('id')['brand'].to_dict()\n",
    "order_brands = order_items.groupby('order_id')['product_id'].apply(lambda ids: [product_id_to_brand.get(pid, None) for pid in ids if pid in product_id_to_brand])\n",
    "brand_pair_counter = Counter()\n",
    "for brands_list in order_brands:\n",
    "    unique_brands = set([brand for brand in brands_list if brand is not None])\n",
    "    for pair in combinations(sorted(unique_brands), 2):\n",
    "        brand_pair_counter[pair] += 1\n",
    "print('\\nTop 10 Product Brand Pairs Bought Together:')\n",
    "for (brand1, brand2), count in brand_pair_counter.most_common(10):\n",
    "    print(f\"  {brand1} + {brand2}: {count:,} orders\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f7b88",
   "metadata": {},
   "source": [
    "### Funnel Analysis: Registration → Event → Order → Repeat Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0e1219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CUSTOMER JOURNEY FUNNEL ANALYSIS ===\n",
      "Available event types: cancel, cart, department, home, product, purchase\n",
      "\n",
      "Event Types by Largest to Smallest:\n",
      "event_type\n",
      "product       844392\n",
      "cart          594666\n",
      "department    594451\n",
      "purchase      181578\n",
      "cancel        125324\n",
      "home           87805\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CUSTOMER JOURNEY FUNNEL:\n",
      "       Funnel Stage   Count  % of Total Users  % of Previous Step\n",
      "0  Registered Users  100000            100.00              100.00\n",
      "1     Site Visitors   79989             79.99               79.99\n",
      "2   Product Viewers   79989             79.99              100.00\n",
      "3       Cart Adders   79989             79.99              100.00\n",
      "4        Purchasers   79989             79.99              100.00\n",
      "5  Repeat Customers   30194             30.19               37.75\n",
      "\n",
      "KEY CONVERSION METRICS:\n",
      "Visitor to Purchase Rate: 100.0%\n",
      "Product View to Purchase Rate: 100.0%\n",
      "Cart to Purchase Rate: 100.0%\n",
      "First Purchase to Repeat Purchase Rate: 37.75%\n"
     ]
    }
   ],
   "source": [
    "# Customer journey funnel analysis: registration to repeat purchase\n",
    "print(\"=== CUSTOMER JOURNEY FUNNEL ANALYSIS ===\")\n",
    "\n",
    "n_registered = users['id'].nunique()\n",
    "event_types = events['event_type'].unique()\n",
    "print(f\"Available event types: {', '.join(event_types)}\")\n",
    "\n",
    "# Most common event types\n",
    "event_counts = events['event_type'].value_counts()\n",
    "print('\\nEvent Types by Largest to Smallest:')\n",
    "print(event_counts)\n",
    "\n",
    "n_visitors = events['user_id'].nunique()\n",
    "n_product_viewers = events[events['event_type'] == 'product']['user_id'].nunique()\n",
    "n_cart_adders = events[events['event_type'] == 'cart']['user_id'].nunique()\n",
    "n_purchasers_events = events[events['event_type'] == 'purchase']['user_id'].nunique()\n",
    "n_purchasers_orders = orders['user_id'].nunique()\n",
    "n_purchasers = n_purchasers_orders\n",
    "repeat_customers = orders.groupby('user_id').size()\n",
    "n_repeat_customers = (repeat_customers > 1).sum()\n",
    "\n",
    "to_pct = lambda x, y: round((x / y * 100), 2) if y > 0 else 0\n",
    "\n",
    "funnel_data = {\n",
    "    'Funnel Stage': ['Registered Users', 'Site Visitors', 'Product Viewers', 'Cart Adders', 'Purchasers', 'Repeat Customers'],\n",
    "    'Count': [n_registered, n_visitors, n_product_viewers, n_cart_adders, n_purchasers, n_repeat_customers],\n",
    "}\n",
    "funnel_df = pd.DataFrame(funnel_data)\n",
    "funnel_df['% of Total Users'] = funnel_df['Count'].apply(lambda x: to_pct(x, n_registered))\n",
    "funnel_df['% of Previous Step'] = [100] + [to_pct(funnel_df.loc[i, 'Count'], funnel_df.loc[i-1, 'Count']) for i in range(1, len(funnel_df))]\n",
    "\n",
    "print(\"\\nCUSTOMER JOURNEY FUNNEL:\")\n",
    "print(funnel_df)\n",
    "\n",
    "print(\"\\nKEY CONVERSION METRICS:\")\n",
    "print(f\"Visitor to Purchase Rate: {to_pct(n_purchasers, n_visitors)}%\")\n",
    "print(f\"Product View to Purchase Rate: {to_pct(n_purchasers, n_product_viewers)}%\")\n",
    "print(f\"Cart to Purchase Rate: {to_pct(n_purchasers, n_cart_adders)}%\")\n",
    "print(f\"First Purchase to Repeat Purchase Rate: {to_pct(n_repeat_customers, n_purchasers)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e68ed",
   "metadata": {},
   "source": [
    "### Geographic Order Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07a72e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 States by Order Count:\n",
      "state\n",
      "Guangdong     6741\n",
      "England       5200\n",
      "California    4705\n",
      "Texas         3167\n",
      "Shanghai      3030\n",
      "Name: order_id, dtype: int64\n",
      "\n",
      "Top 5 Countries by Order Count:\n",
      "country\n",
      "China            42245\n",
      "United States    28204\n",
      "Brasil           18249\n",
      "South Korea       6583\n",
      "France            6028\n",
      "Name: order_id, dtype: int64\n",
      "\n",
      "Top 5 Cities by Order Count:\n",
      "city\n",
      "Shanghai    3187\n",
      "Beijing     2428\n",
      "Seoul       1831\n",
      "Shenzhen    1641\n",
      "Dongguan    1240\n",
      "Name: order_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Geographic order analysis: Top states, countries, and cities by order count\n",
    "orders_users = orders.merge(users, left_on='user_id', right_on='id', how='left')\n",
    "geo_rev_state = orders_users.groupby('state')['order_id'].count().sort_values(ascending=False).head(5)\n",
    "geo_rev_country = orders_users.groupby('country')['order_id'].count().sort_values(ascending=False).head(5)\n",
    "geo_rev_city = orders_users.groupby('city')['order_id'].count().sort_values(ascending=False).head(5)\n",
    "print('\\nTop 5 States by Order Count:')\n",
    "print(geo_rev_state)\n",
    "print('\\nTop 5 Countries by Order Count:')\n",
    "print(geo_rev_country)\n",
    "print('\\nTop 5 Cities by Order Count:')\n",
    "print(geo_rev_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66c3d5",
   "metadata": {},
   "source": [
    "### Customer Lifetime Value (CLV) Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a65f4202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Segments by CLV:\n",
      "sale_price\n",
      "Low          20010\n",
      "Medium       19985\n",
      "High         19997\n",
      "Very High    19997\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CLV segmentation\n",
    "clv = order_items.merge(orders[['order_id','user_id']], left_on='order_id', right_on='order_id')\n",
    "clv = clv.groupby('user_id_x')['sale_price'].sum()\n",
    "clv_bins = pd.qcut(clv, q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "clv_summary = clv_bins.value_counts().sort_index()\n",
    "print('Customer Segments by CLV:')\n",
    "print(clv_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64835e9a",
   "metadata": {},
   "source": [
    "### Returns Analysis: Return Rates by Product and Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f32930e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Products by Return Count and Percentage:\n",
      "  G. Fiorelli 3 Button Men Suit Solid Dark Navy Super 150's Premium Quality Textile: 6 returns (60.00% of orders)\n",
      "  Kanu Surf Men's Epic Swim Trunk: 6 returns (37.50% of orders)\n",
      "  Titanium Smart Money Clip: 6 returns (60.00% of orders)\n",
      "  BraBag Buxom 2109 The Victoria Moulded Travel Bag (D to G cups) 2109: 5 returns (41.67% of orders)\n",
      "  McGinn Women's Harmony Jacket: 5 returns (62.50% of orders)\n",
      "\n",
      "Top 5 Categories by Return Count and Percentage:\n",
      "  Intimates: 1,416 returns (10.41% of orders)\n",
      "  Jeans: 1,242 returns (9.67% of orders)\n",
      "  Tops & Tees: 1,145 returns (9.61% of orders)\n",
      "  Fashion Hoodies & Sweatshirts: 1,143 returns (9.80% of orders)\n",
      "  Shorts: 1,122 returns (10.06% of orders)\n",
      "\n",
      "Top 5 Brands by Return Count and Percentage:\n",
      "  Allegra K: 609 returns (9.74% of orders)\n",
      "  Calvin Klein: 296 returns (9.44% of orders)\n",
      "  Carhartt: 239 returns (9.47% of orders)\n",
      "  Hanes: 228 returns (11.59% of orders)\n",
      "  Nautica: 173 returns (9.72% of orders)\n",
      "\n",
      "Top 5 Brands by Return Percentage (only brands with total sold > Q2):\n",
      "  Dopp: 7 returns out of 23 orders (30.43%)\n",
      "  Storus Corporation: 6 returns out of 20 orders (30.00%)\n",
      "  Jonathan Quale: 5 returns out of 17 orders (29.41%)\n",
      "  Ocean Current: 5 returns out of 17 orders (29.41%)\n",
      "  DECKY: 5 returns out of 17 orders (29.41%)\n"
     ]
    }
   ],
   "source": [
    "# Return rates by product and category with percentage of returns\n",
    "\n",
    "# Top 5 products by return count and percentage\n",
    "order_items_returns = order_items[order_items['returned_at'].notna()]\n",
    "returns_by_product = order_items_returns.groupby('product_id').size().sort_values(ascending=False).head(5)\n",
    "\n",
    "print('Top 5 Products by Return Count and Percentage:')\n",
    "for pid, count in returns_by_product.items():\n",
    "    name = products.loc[products['id'] == pid, 'name'].values[0]\n",
    "    total_orders = order_items[order_items['product_id'] == pid].shape[0]\n",
    "    pct = (count / total_orders * 100) if total_orders else 0\n",
    "    print(f\"  {name}: {count:,} returns ({pct:.2f}% of orders)\")\n",
    "\n",
    "# Top 5 categories by return count and percentage\n",
    "returns_by_category = order_items_returns.merge(products[['id','category']], left_on='product_id', right_on='id')\n",
    "returns_by_category_count = returns_by_category.groupby('category').size().sort_values(ascending=False).head(5)\n",
    "\n",
    "print('\\nTop 5 Categories by Return Count and Percentage:')\n",
    "for cat, count in returns_by_category_count.items():\n",
    "    total_orders = order_items.merge(products[['id','category']], left_on='product_id', right_on='id')\n",
    "    total_cat_orders = total_orders[total_orders['category'] == cat].shape[0]\n",
    "    pct = (count / total_cat_orders * 100) if total_cat_orders else 0\n",
    "    print(f\"  {cat}: {count:,} returns ({pct:.2f}% of orders)\")\n",
    "\n",
    "# Top 5 brands by return count and percentage\n",
    "returns_by_brand = order_items_returns.merge(products[['id', 'brand']], left_on='product_id', right_on='id')\n",
    "returns_by_brand_count = returns_by_brand.groupby('brand').size().sort_values(ascending=False).head(5)\n",
    "\n",
    "print('\\nTop 5 Brands by Return Count and Percentage:')\n",
    "for brand, count in returns_by_brand_count.items():\n",
    "    total_orders = order_items.merge(products[['id', 'brand']], left_on='product_id', right_on='id')\n",
    "    total_brand_orders = total_orders[total_orders['brand'] == brand].shape[0]\n",
    "    pct = (count / total_brand_orders * 100) if total_brand_orders else 0\n",
    "    print(f\"  {brand}: {count:,} returns ({pct:.2f}% of orders)\")\n",
    "\n",
    "# Top 5 brands by return percentage (only brands with total product sold > q2 overall)\n",
    "brand_total_orders = order_items.merge(products[['id', 'brand']], left_on='product_id', right_on='id')\n",
    "brand_order_counts = brand_total_orders['brand'].value_counts()\n",
    "returns_by_brand_counts = returns_by_brand['brand'].value_counts()\n",
    "\n",
    "# Compute q2 (median) for brand order counts\n",
    "q2 = brand_order_counts.median()\n",
    "\n",
    "# Only keep brands with total orders > q2\n",
    "brands_above_q2 = brand_order_counts[brand_order_counts > q2].index\n",
    "filtered_returns_by_brand_counts = returns_by_brand_counts[returns_by_brand_counts.index.isin(brands_above_q2)]\n",
    "filtered_brand_order_counts = brand_order_counts[brand_order_counts.index.isin(brands_above_q2)]\n",
    "\n",
    "brand_return_pct = (filtered_returns_by_brand_counts / filtered_brand_order_counts * 100).dropna()\n",
    "brand_return_pct = brand_return_pct.sort_values(ascending=False).head(5)\n",
    "\n",
    "print('\\nTop 5 Brands by Return Percentage (only brands with total sold > Q2):')\n",
    "for brand, pct in brand_return_pct.items():\n",
    "    count = filtered_returns_by_brand_counts.get(brand, 0)\n",
    "    total = filtered_brand_order_counts.get(brand, 0)\n",
    "    print(f\"  {brand}: {count:,} returns out of {total:,} orders ({pct:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
