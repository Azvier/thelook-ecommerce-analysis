{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfde04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f61687",
   "metadata": {},
   "source": [
    "# Database Connection\n",
    "\n",
    "Connect to the PostgreSQL database with the provided credentials. SQLAlchemy will be used for database operations in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ae5e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLAlchemy engine created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Connect to PostgreSQL using SQLAlchemy\n",
    "try:\n",
    "    engine = create_engine(\n",
    "        'postgresql://azalea:azalea@localhost:5433/thelook_db'\n",
    "    )\n",
    "    # Test the connection\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text('SELECT 1'))\n",
    "    print(\"SQLAlchemy engine created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to database: {e}\")\n",
    "    print(\"\\nPlease ensure that PostgreSQL database is running\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2248b7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['users',\n",
       " 'events',\n",
       " 'orders',\n",
       " 'distribution_centers',\n",
       " 'products',\n",
       " 'inventory_items',\n",
       " 'order_items']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe97d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables read successfully.\n"
     ]
    }
   ],
   "source": [
    "# Read each table into a pandas DataFrame, assigning to individual variables\n",
    "users = pd.read_sql_table('users', engine)\n",
    "events = pd.read_sql_table('events', engine)\n",
    "orders = pd.read_sql_table('orders', engine)\n",
    "distribution_centers = pd.read_sql_table('distribution_centers', engine)\n",
    "products = pd.read_sql_table('products', engine)\n",
    "inventory_items = pd.read_sql_table('inventory_items', engine)\n",
    "order_items = pd.read_sql_table('order_items', engine)\n",
    "\n",
    "print('All tables read successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b9c9455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLAlchemy engine disposed.\n"
     ]
    }
   ],
   "source": [
    "# Close all database connections\n",
    "try:\n",
    "    if 'engine' in locals():\n",
    "        engine.dispose()\n",
    "        print(\"SQLAlchemy engine disposed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error disposing SQLAlchemy engine: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a20e54",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a76d0",
   "metadata": {},
   "source": [
    "## Data Overview and Quality Assessment\n",
    "\n",
    "Understanding the structure and quality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7d2a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shapes:\n",
      "Users: (100000, 16)\n",
      "Events: (2428216, 13)\n",
      "Orders: (125278, 9)\n",
      "Order Items: (181578, 11)\n",
      "Products: (29120, 9)\n",
      "Inventory Items: (490176, 12)\n",
      "Distribution Centers: (10, 5)\n",
      "\n",
      "Memory Usage (MB):\n",
      "users: 67.77 MB\n",
      "events: 1362.93 MB\n",
      "orders: 19.50 MB\n",
      "order_items: 23.77 MB\n",
      "products: 10.62 MB\n",
      "inventory_items: 189.79 MB\n",
      "distribution_centers: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Basic information about each dataset\n",
    "print(\"Dataset Shapes:\")\n",
    "print(f\"Users: {users.shape}\")\n",
    "print(f\"Events: {events.shape}\")\n",
    "print(f\"Orders: {orders.shape}\")\n",
    "print(f\"Order Items: {order_items.shape}\")\n",
    "print(f\"Products: {products.shape}\")\n",
    "print(f\"Inventory Items: {inventory_items.shape}\")\n",
    "print(f\"Distribution Centers: {distribution_centers.shape}\")\n",
    "\n",
    "# Memory usage\n",
    "print(\"\\nMemory Usage (MB):\")\n",
    "for name, df in [('users', users), ('events', events), ('orders', orders), \n",
    "                 ('order_items', order_items), ('products', products), \n",
    "                 ('inventory_items', inventory_items), ('distribution_centers', distribution_centers)]:\n",
    "    print(f\"{name}: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d37a6",
   "metadata": {},
   "source": [
    "The datasets are large and detailed, especially `events`, which uses the most memory. Most tables have hundreds of thousands of rows, except for `distribution_centers`, which is small. Efficient filtering is important for analysis due to the size of the largest tables. Overall, the data is comprehensive and ready for business analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd48311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== USERS DATA QUALITY ===\n",
      "Shape: (100000, 16)\n",
      "Duplicates: 0\n",
      "\n",
      "No missing values found!\n",
      "\n",
      "Data Types:\n",
      "object                 11\n",
      "int64                   2\n",
      "float64                 2\n",
      "datetime64[ns, UTC]     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ORDERS DATA QUALITY ===\n",
      "Shape: (125278, 9)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "              Missing Count  Missing %\n",
      "returned_at          112833  90.066093\n",
      "delivered_at          81219  64.831016\n",
      "shipped_at            43559  34.769872\n",
      "\n",
      "Data Types:\n",
      "datetime64[ns, UTC]    4\n",
      "int64                  3\n",
      "object                 2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ORDER_ITEMS DATA QUALITY ===\n",
      "Shape: (181578, 11)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "              Missing Count  Missing %\n",
      "returned_at          163615  90.107282\n",
      "delivered_at         117660  64.798599\n",
      "shipped_at            63029  34.711804\n",
      "\n",
      "Data Types:\n",
      "int64                  5\n",
      "datetime64[ns, UTC]    4\n",
      "object                 1\n",
      "float64                1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== PRODUCTS DATA QUALITY ===\n",
      "Shape: (29120, 9)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "       Missing Count  Missing %\n",
      "brand             24   0.082418\n",
      "name               2   0.006868\n",
      "\n",
      "Data Types:\n",
      "object     5\n",
      "int64      2\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== INVENTORY_ITEMS DATA QUALITY ===\n",
      "Shape: (490176, 12)\n",
      "Duplicates: 0\n",
      "\n",
      "Missing Values:\n",
      "         Missing Count  Missing %\n",
      "sold_at         308598  62.956571\n",
      "\n",
      "Data Types:\n",
      "object                 5\n",
      "int64                  3\n",
      "datetime64[ns, UTC]    2\n",
      "float64                2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data quality assessment\n",
    "import numpy as np\n",
    "\n",
    "def data_quality_summary(df, name):\n",
    "    print(f\"\\n=== {name.upper()} DATA QUALITY ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    })\n",
    "    missing_info = missing_info[missing_info['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    if not missing_info.empty:\n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(missing_info)\n",
    "    else:\n",
    "        print(\"\\nNo missing values found!\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    return missing_info\n",
    "\n",
    "# Check each dataset\n",
    "for name, df in [('users', users), ('orders', orders), ('order_items', order_items), \n",
    "                 ('products', products), ('inventory_items', inventory_items)]:\n",
    "    data_quality_summary(df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a66311",
   "metadata": {},
   "source": [
    "The data quality assessment shows:\n",
    "\n",
    "- **Missing Values**: Minimal missing data across most tables. Geographic fields (latitude, longitude) and fulfillment dates (shipped_at, delivered_at) show expected gaps. Core business fields (IDs, prices, essential dates) are complete.\n",
    "\n",
    "- **Data Types**: Appropriate data types throughout - numeric fields for calculations, text fields for categories, and timestamp fields for time series analysis.\n",
    "\n",
    "- **Duplicates**: No duplicate records found, indicating proper data deduplication.\n",
    "\n",
    "- **Overall**: The data quality is excellent for business analysis with minimal cleaning required. The completeness of critical fields enables comprehensive examination of customer behavior, product performance, and operational metrics.\n",
    "\n",
    "Given the missing data patterns, we need to investigate whether these are truly missing values or represent natural business states (e.g., orders not yet shipped, products not yet sold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e2498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MISSING DATA MANAGEMENT STRATEGY ===\n",
      "\n",
      "1. ORDERS TABLE - Analyzing fulfillment workflow missing data\n",
      "\n",
      "Missing fulfillment dates by order status:\n",
      "            Missing_Shipped  Missing_Delivered  Missing_Returned  Total_Orders\n",
      "status                                                                        \n",
      "Cancelled             18714              18714             18714         18714\n",
      "Complete                  0                  0             31614         31614\n",
      "Processing            24845              24845             24845         24845\n",
      "Returned                  0                  0                 0         12445\n",
      "Shipped                   0              37660             37660         37660\n",
      "\n",
      "Missing fulfillment dates by order status (with percentages):\n",
      "            Total_Orders  Missing_Shipped  Missing_Shipped_Pct  \\\n",
      "status                                                           \n",
      "Cancelled          18714            18714                100.0   \n",
      "Complete           31614                0                  0.0   \n",
      "Processing         24845            24845                100.0   \n",
      "Returned           12445                0                  0.0   \n",
      "Shipped            37660                0                  0.0   \n",
      "\n",
      "            Missing_Delivered  Missing_Delivered_Pct  Missing_Returned  \\\n",
      "status                                                                   \n",
      "Cancelled               18714                  100.0             18714   \n",
      "Complete                    0                    0.0             31614   \n",
      "Processing              24845                  100.0             24845   \n",
      "Returned                    0                    0.0                 0   \n",
      "Shipped                 37660                  100.0             37660   \n",
      "\n",
      "            Missing_Returned_Pct  \n",
      "status                            \n",
      "Cancelled                  100.0  \n",
      "Complete                   100.0  \n",
      "Processing                 100.0  \n",
      "Returned                     0.0  \n",
      "Shipped                    100.0  \n"
     ]
    }
   ],
   "source": [
    "# Missing Data Management and Analysis\n",
    "print(\"=== MISSING DATA MANAGEMENT STRATEGY ===\")\n",
    "print(\"\\n1. ORDERS TABLE - Analyzing fulfillment workflow missing data\")\n",
    "\n",
    "# Check if missing dates are due to order status rather than true missing data\n",
    "order_status_analysis = orders.groupby('status').agg({\n",
    "    'shipped_at': lambda x: x.isnull().sum(),\n",
    "    'delivered_at': lambda x: x.isnull().sum(), \n",
    "    'returned_at': lambda x: x.isnull().sum()\n",
    "}).astype(int)\n",
    "\n",
    "order_status_analysis.columns = ['Missing_Shipped', 'Missing_Delivered', 'Missing_Returned']\n",
    "order_status_analysis['Total_Orders'] = orders.groupby('status').size()\n",
    "\n",
    "print(\"\\nMissing fulfillment dates by order status:\")\n",
    "print(order_status_analysis)\n",
    "\n",
    "# Calculate percentages\n",
    "for col in ['Missing_Shipped', 'Missing_Delivered', 'Missing_Returned']:\n",
    "    order_status_analysis[f'{col}_Pct'] = (order_status_analysis[col] / order_status_analysis['Total_Orders'] * 100).round(1)\n",
    "\n",
    "print(\"\\nMissing fulfillment dates by order status (with percentages):\")\n",
    "print(order_status_analysis[['Total_Orders', 'Missing_Shipped', 'Missing_Shipped_Pct', \n",
    "                           'Missing_Delivered', 'Missing_Delivered_Pct',\n",
    "                           'Missing_Returned', 'Missing_Returned_Pct']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0096eff",
   "metadata": {},
   "source": [
    "Analysis reveals that missing fulfillment dates represent the natural order processing workflow rather than data quality issues. Each status shows expected patterns:\n",
    "\n",
    "- **Cancelled orders**: 100% missing all dates (never entered fulfillment)\n",
    "- **Processing orders**: 100% missing all dates (awaiting shipment)\n",
    "- **Shipped orders**: Have shipping dates but 100% missing delivered/returned dates\n",
    "- **Complete orders**: No missing shipping/delivery dates, but 100% missing return dates\n",
    "- **Returned orders**: Complete data across all date fields\n",
    "\n",
    "These patterns confirm the database accurately tracks order lifecycle stages, with \"missing\" values actually serving as meaningful status indicators rather than data deficiencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e889b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. ORDER_ITEMS TABLE - Analyzing item-level fulfillment data\n",
      "\n",
      "Missing fulfillment dates in order_items by status:\n",
      "            Missing_Shipped  Missing_Delivered  Missing_Returned  Total_Items\n",
      "status                                                                       \n",
      "Cancelled             27190              27190             27190        27190\n",
      "Complete                  0                  0             45955        45955\n",
      "Processing            35839              35839             35839        35839\n",
      "Returned                  0                  0                 0        17963\n",
      "Shipped                   0              54631             54631        54631\n",
      "\n",
      "\n",
      "3. INVENTORY_ITEMS TABLE - Analyzing sold_at missing data\n",
      "\n",
      "Inventory items sold status:\n",
      "   Total_Inventory_Items  Items_with_sold_at  Items_NOT_sold  \\\n",
      "0                 490176              181578          308598   \n",
      "\n",
      "   Percentage_Unsold  \n",
      "0               63.0  \n",
      "\n",
      "Items with sold_at: 181,578\n",
      "Items NOT sold: 308,598\n",
      "Sum of both: 490,176\n",
      "Total inventory items (verification): 490,176\n",
      "\n",
      "Verification: Sum equals total inventory items: True\n"
     ]
    }
   ],
   "source": [
    "print(\"2. ORDER_ITEMS TABLE - Analyzing item-level fulfillment data\")\n",
    "\n",
    "# Analyze missing data in order_items by status\n",
    "order_items_analysis = order_items.groupby('status').agg({\n",
    "    'shipped_at': lambda x: x.isnull().sum(),\n",
    "    'delivered_at': lambda x: x.isnull().sum(),\n",
    "    'returned_at': lambda x: x.isnull().sum()\n",
    "}).astype(int)\n",
    "\n",
    "order_items_analysis.columns = ['Missing_Shipped', 'Missing_Delivered', 'Missing_Returned']\n",
    "order_items_analysis['Total_Items'] = order_items.groupby('status').size()\n",
    "\n",
    "print(\"\\nMissing fulfillment dates in order_items by status:\")\n",
    "print(order_items_analysis)\n",
    "\n",
    "# Check if order_items missing data aligns with parent orders\n",
    "print(\"\\n\\n3. INVENTORY_ITEMS TABLE - Analyzing sold_at missing data\")\n",
    "\n",
    "# For inventory_items, missing sold_at likely means items haven't been sold yet\n",
    "inventory_sold_analysis = pd.DataFrame({\n",
    "    'Total_Inventory_Items': [len(inventory_items)],\n",
    "    'Items_with_sold_at': [inventory_items['sold_at'].notna().sum()],\n",
    "    'Items_NOT_sold': [inventory_items['sold_at'].isnull().sum()],\n",
    "    'Percentage_Unsold': [(inventory_items['sold_at'].isnull().sum() / len(inventory_items) * 100).round(1)]\n",
    "})\n",
    "\n",
    "print(\"\\nInventory items sold status:\")\n",
    "print(inventory_sold_analysis)\n",
    "\n",
    "# Calculate and print the sum of Items_with_sold_at and Items_NOT_sold\n",
    "items_sold = inventory_sold_analysis['Items_with_sold_at'].iloc[0]\n",
    "items_not_sold = inventory_sold_analysis['Items_NOT_sold'].iloc[0]\n",
    "total_sum = items_sold + items_not_sold\n",
    "\n",
    "print(f\"\\nItems with sold_at: {items_sold:,}\")\n",
    "print(f\"Items NOT sold: {items_not_sold:,}\")\n",
    "print(f\"Sum of both: {total_sum:,}\")\n",
    "print(f\"Total inventory items (verification): {len(inventory_items):,}\")\n",
    "print(f\"\\nVerification: Sum equals total inventory items: {total_sum == len(inventory_items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0480df3",
   "metadata": {},
   "source": [
    "**Order Items Processing Flow**\n",
    "- **Cancelled orders**: 100% missing all fulfillment dates (27,190 items)\n",
    "- **Processing orders**: 100% missing all dates as they haven't been shipped (35,839 items)\n",
    "- **Shipped orders**: Have shipping dates but await delivery (54,631 items)\n",
    "- **Complete orders**: Full shipping and delivery information (45,955 items) \n",
    "- **Returned items**: Complete history across all date fields (17,963 items)\n",
    "\n",
    "**Inventory Management Status**\n",
    "- **37% of inventory sold**: 181,578 items have sold_at dates\n",
    "- **63% of inventory unsold**: 308,598 items remain available in stock\n",
    "- **Total inventory**: 490,176 items\n",
    "\n",
    "These patterns indicate that missing date values represent meaningful business states in the order processing and inventory management workflows rather than data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0255199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. PRODUCTS TABLE - Analyzing missing product data\n",
      "\n",
      "Missing data in products table:\n",
      "       Missing_Count  Missing_Percentage\n",
      "brand             24                0.08\n",
      "name               2                0.01\n",
      "\n",
      "Analyzing patterns in missing product data:\n",
      "\n",
      "brand missing by category (top 5):\n",
      "category\n",
      "Intimates            4\n",
      "Tops & Tees          4\n",
      "Outerwear & Coats    3\n",
      "Swim                 2\n",
      "Accessories          2\n",
      "Name: brand, dtype: int64\n",
      "\n",
      "brand missing by department:\n",
      "department\n",
      "Men      12\n",
      "Women    12\n",
      "Name: brand, dtype: int64\n",
      "\n",
      "name missing by category (top 5):\n",
      "category\n",
      "Intimates            1\n",
      "Outerwear & Coats    1\n",
      "Name: name, dtype: int64\n",
      "\n",
      "name missing by department:\n",
      "department\n",
      "Men      1\n",
      "Women    1\n",
      "Name: name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"4. PRODUCTS TABLE - Analyzing missing product data\")\n",
    "\n",
    "# Analyze missing data patterns in products\n",
    "products_missing = products.isnull().sum()\n",
    "products_missing_pct = (products_missing / len(products) * 100).round(2)\n",
    "\n",
    "products_missing_df = pd.DataFrame({\n",
    "    'Missing_Count': products_missing,\n",
    "    'Missing_Percentage': products_missing_pct\n",
    "})\n",
    "products_missing_df = products_missing_df[products_missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"\\nMissing data in products table:\")\n",
    "print(products_missing_df)\n",
    "\n",
    "# Check if missing data correlates with certain categories or departments\n",
    "if not products_missing_df.empty:\n",
    "    print(\"\\nAnalyzing patterns in missing product data:\")\n",
    "    \n",
    "    # Check missing data by category\n",
    "    for col in products_missing_df.index:\n",
    "        if col in products.columns:\n",
    "            missing_by_category = products.groupby('category')[col].apply(lambda x: x.isnull().sum())\n",
    "            missing_by_category = missing_by_category[missing_by_category > 0].sort_values(ascending=False)\n",
    "            \n",
    "            if not missing_by_category.empty:\n",
    "                print(f\"\\n{col} missing by category (top 5):\")\n",
    "                print(missing_by_category.head())\n",
    "            \n",
    "            # Check missing data by department\n",
    "            missing_by_dept = products.groupby('department')[col].apply(lambda x: x.isnull().sum())\n",
    "            missing_by_dept = missing_by_dept[missing_by_dept > 0].sort_values(ascending=False)\n",
    "            \n",
    "            if not missing_by_dept.empty:\n",
    "                print(f\"\\n{col} missing by department:\")\n",
    "                print(missing_by_dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8484b",
   "metadata": {},
   "source": [
    "The products table shows excellent data quality with very limited missing information. The few missing values are evenly distributed across departments, suggesting random omissions rather than systematic data issues. table shows excellent data quality with very limited missing information. The few missing values are evenly distributed across departments, suggesting random omissions rather than systematic data issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
